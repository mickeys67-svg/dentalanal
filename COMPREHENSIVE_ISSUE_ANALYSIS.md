# ğŸ”´ DentalAnal ì‹œìŠ¤í…œ - 10ê°€ì§€ í•µì‹¬ ë¬¸ì œì  ë¶„ì„

> **ëŒ€ê·œëª¨ ê¸°ìˆ  ë””ë²„ê¹… ë¶„ì„ ë³´ê³ ì„œ**
>
> ì‘ì„±ì¼: 2026-02-20
> ë¶„ì„ ë²”ìœ„: í”„ë¡ íŠ¸ì—”ë“œ, ë°±ì—”ë“œ, ë°°í¬ íŒŒì´í”„ë¼ì¸, ë°ì´í„° íë¦„
> ì‹¬ê°ë„: ğŸ”´ 5ê°œ (ë†’ìŒ), ğŸŸ¡ 3ê°œ (ì¤‘ê°„), ğŸŸ  2ê°œ (ë‚®ìŒ)

---

## ğŸ“Š ë¬¸ì œì  ìš”ì•½ ë§¤íŠ¸ë¦­ìŠ¤

| # | ë¬¸ì œì  | ì‹¬ê°ë„ | ì¹´í…Œê³ ë¦¬ | ì˜í–¥ ë²”ìœ„ | ì‚¬ìš©ì ì²´ê° |
|---|--------|--------|----------|----------|-----------|
| **1** | NEXT_PUBLIC_API_URL ë¯¸ì„¤ì • | ğŸ”´ ë†’ìŒ | ë°°í¬ | í”„ë¡ íŠ¸ì—”ë“œ ì „ì²´ | API í˜¸ì¶œ ì‹¤íŒ¨ |
| **2** | Docker ëŸ°íƒ€ì„ í™˜ê²½ë³€ìˆ˜ ë¬´íš¨í™” | ğŸ”´ ë†’ìŒ | ë°°í¬ | Cloud Run | ê³ ì • URLë¡œ ì„¤ì •ë¨ |
| **3** | API ë¼ìš°íŒ… ê²½ë¡œ ë¶ˆì¼ì¹˜ | ğŸŸ¡ ì¤‘ê°„ | ë¼ìš°íŒ… | /status í”„ë¦¬í”½ìŠ¤ | 404 ì—ëŸ¬ |
| **4** | UUID íƒ€ì… ê²€ì¦ ì‹¤íŒ¨ | ğŸŸ¡ ì¤‘ê°„ | ë°ì´í„° ê²€ì¦ | í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ | 422 ì—ëŸ¬ |
| **5** | ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ê³ ì • | ğŸŸ¡ ì¤‘ê°„ | ë¹„ë™ê¸° ì²˜ë¦¬ | SetupWizard | ë°ì´í„° ë¯¸í‘œì‹œ |
| **6** | ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì—ëŸ¬ ë¬´ì‹œ | ğŸ”´ ë†’ìŒ | ì—ëŸ¬ ì²˜ë¦¬ | ìŠ¤í¬ë˜í•‘ íŒŒì´í”„ë¼ì¸ | ë¬¸ì œ ì¸ì§€ ë¶ˆê°€ |
| **7** | ì¸ì¦ ë¯¸ê²€ì¦ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸ | ğŸ”´ ë†’ìŒ | ë³´ì•ˆ | /api/v1/scrape/* | ë¬´ë‹¨ ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥ |
| **8** | BackgroundTasks ì‹ ë¢°ì„± ë¶€ì¡± | ğŸ”´ ë†’ìŒ | ì•„í‚¤í…ì²˜ | ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… | ì‘ì—… ì†ì‹¤ ê°€ëŠ¥ |
| **9** | ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¶”ì  ë¶ˆê°€ | ğŸŸ  ë‚®ìŒ | ëª¨ë‹ˆí„°ë§ | ì‚¬ìš©ì ê²½í—˜ | ì§„í–‰ ìƒí™© ë¯¸í‘œì‹œ |
| **10** | í´ë¼ì´ì–¸íŠ¸ID ê²€ì¦ ì§€ì—° | ğŸŸ  ë‚®ìŒ | UX | SetupWizard | ì§€ì—°ëœ ì—ëŸ¬ í”¼ë“œë°± |

---

## ğŸ”´ ë¬¸ì œì  1: NEXT_PUBLIC_API_URL ë¯¸ì„¤ì •

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/lib/api.ts` (ë¼ì¸ 28)
- **ë°°í¬ ì„¤ì •**: `.github/workflows/deploy.yml` (ë¼ì¸ 65)

### í˜„ì¬ ì½”ë“œ
```typescript
// frontend/src/lib/api.ts
const API_BASE_URL = process.env.NEXT_PUBLIC_API_URL || 'https://BACKEND_URL_NOT_SET';

export const api = axios.create({
    baseURL: API_BASE_URL,
    headers: { 'Content-Type': 'application/json' },
});
```

### ê·¼ë³¸ ì›ì¸
- **Next.jsì˜ ì‘ë™ ì›ë¦¬**: í™˜ê²½ë³€ìˆ˜ëŠ” ë¹Œë“œ íƒ€ì„ì— ë²ˆë“¤ë§ë¨
- **Dockerfile ë¬¸ì œ**: ARGë¡œ ì •ì˜ë§Œ í•˜ê³  ENVë¡œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
```dockerfile
# frontend/Dockerfile (ë¼ì¸ 23-24)
ARG NEXT_PUBLIC_API_URL
ENV NEXT_PUBLIC_API_URL=$NEXT_PUBLIC_API_URL  # â† ë¹Œë“œ íƒ€ì„ í•„ìˆ˜
```

- **GitHub Actions ë¬¸ì œ**:
```yaml
# .github/workflows/deploy.yml (ë¼ì¸ 65)
docker build --build-arg NEXT_PUBLIC_API_URL=${{ steps.deploy_backend.outputs.url }} ...
# í•˜ì§€ë§Œ NEXT_PUBLIC_API_URLì´ ë¹Œë“œ ì¸ìë¡œ ì „ë‹¬ë˜ëŠ” ë™ì‹œì—
# í”„ë¡ íŠ¸ì—”ë“œëŠ” ì´ë¯¸ ì´ì „ ë¹Œë“œì—ì„œ 'BACKEND_URL_NOT_SET'ìœ¼ë¡œ ì»´íŒŒì¼ë¨
```

### ë¬¸ì œ ë°œìƒ ì‹œë‚˜ë¦¬ì˜¤

```
ë°°í¬ ìˆœì„œ:
1ï¸âƒ£ Backend ë¹Œë“œ & ë°°í¬ â†’ URL íšë“: https://dentalanal-864421937037.us-west1.run.app
2ï¸âƒ£ Frontend ë¹Œë“œ ì¸ì: NEXT_PUBLIC_API_URL=https://dentalanal-864421937037.us-west1.run.app
3ï¸âƒ£ Next.js ë¹Œë“œ ì‹œì‘:
   - pages/page.tsx ë³€í™˜
   - lib/api.ts ë³€í™˜
   - process.env.NEXT_PUBLIC_API_URL ì¡°íšŒ
   âŒ ë¹Œë“œ í™˜ê²½ì— NEXT_PUBLIC_API_URLì´ ì—†ìœ¼ë©´
   âŒ ëŒ€ì‹  'BACKEND_URL_NOT_SET'ë¡œ í•˜ë“œì½”ë”©ë¨
4ï¸âƒ£ ìµœì¢… íŒŒì¼:
   const API_BASE_URL = 'https://BACKEND_URL_NOT_SET';
   // ë˜ëŠ” ë„ë©”ì¸ ì£¼ì†Œë¡œ ì„¤ì •ë¨ (build-argê°€ ì œëŒ€ë¡œ ì „ë‹¬ëœ ê²½ìš°)
```

### ì˜í–¥
```javascript
// ëª¨ë“  API í˜¸ì¶œ ì‹¤íŒ¨
await api.get('/api/v1/status/status')
// ìš”ì²­ URL: https://BACKEND_URL_NOT_SET/api/v1/status/status
// ê²°ê³¼: ERR_NAME_NOT_RESOLVED ë˜ëŠ” CORS ì—ëŸ¬
```

### í•´ê²° ë°©ë²•
```dockerfile
# frontend/Dockerfile - ìˆ˜ì •
ARG NEXT_PUBLIC_API_URL
ENV NEXT_PUBLIC_API_URL=$NEXT_PUBLIC_API_URL

RUN npm run build  # â† ì´ ì‹œì ì— ENVê°€ í•„ìˆ˜
```

---

## ğŸ”´ ë¬¸ì œì  2: Docker ëŸ°íƒ€ì„ í™˜ê²½ë³€ìˆ˜ ë¬´íš¨í™”

### ìœ„ì¹˜
- **íŒŒì¼**: `.github/workflows/deploy.yml` (ë¼ì¸ 70)
- **ë°°í¬ ëŒ€ìƒ**: Cloud Run (í”„ë¡ íŠ¸ì—”ë“œ)

### í˜„ì¬ ì½”ë“œ
```yaml
# Deploy Frontend to Cloud Run
- name: Deploy Frontend to Cloud Run
  uses: google-github-actions/deploy-cloudrun@v2
  with:
    service: dentalanal
    image: ...
    env_vars: |
      NEXT_PUBLIC_API_URL=${{ steps.deploy_backend.outputs.url }}
    flags: "--allow-unauthenticated --port=8080"
```

### ê·¼ë³¸ ì›ì¸
**Next.jsëŠ” Static Export ëª¨ë“œì—ì„œ ì‘ë™**
- ëª¨ë“  í˜ì´ì§€ê°€ ë¹Œë“œ íƒ€ì„ì— HTMLë¡œ ë³€í™˜ë¨
- ëŸ°íƒ€ì„ í™˜ê²½ë³€ìˆ˜ëŠ” íš¨ê³¼ ì—†ìŒ
- **ì •í™•íˆëŠ”**: `process.env.NEXT_PUBLIC_API_URL`ëŠ” ë²ˆë“¤ì— ì´ë¯¸ í¬í•¨ë¨

### ë¹„êµ: Node.js ì•± vs Next.js Static

```typescript
// âŒ Next.js (static export)
// ë¹Œë“œ íƒ€ì„: process.env.NEXT_PUBLIC_API_URL â†’ ë²ˆë“¤ì— ì§ì ‘ ì‘ì„±ë¨
const API_URL = process.env.NEXT_PUBLIC_API_URL;
// ëŸ°íƒ€ì„ í™˜ê²½ë³€ìˆ˜: ë¬´ì‹œë¨

// âœ… Node.js (Express)
// ëŸ°íƒ€ì„: process.env.API_URL â†’ ì„œë²„ ì‹œì‘ ì‹œ ì½ìŒ
const API_URL = process.env.API_URL;
```

### Dockerfile ë¶„ì„
```dockerfile
# frontend/Dockerfile (ë¼ì¸ 49)
RUN npm run build  # â† ì´ ì‹œì ì— NEXT_PUBLIC_API_URLì´ í•„ìš”
# ì´í›„ë¡œ ENV ì„¤ì •í•´ë„ ì´ë¯¸ ë¹Œë“œë¨

# ëŸ°íƒ€ì„ì— ë‹¤ì‹œ ì„¤ì •í•´ë„...
ENV NEXT_PUBLIC_API_URL=$NEXT_PUBLIC_API_URL  # â† íš¨ê³¼ ì—†ìŒ
# ì´ë¯¸ ë³€í™˜ëœ HTML/JSì—ëŠ” ì´ë¯¸ ì´ì „ ê°’ì´ í•˜ë“œì½”ë”©ë¨
```

### ê²°ê³¼
```
ë°°í¬ íƒ€ì„ë¼ì¸:
ğŸ”µ Backend URL: https://dentalanal-864421937037.us-west1.run.app (ì •ìƒ)
ğŸ”µ Frontend ë¹Œë“œ ARG: NEXT_PUBLIC_API_URL=https://... (ì˜¬ë°”ë¦„)
âŒ í•˜ì§€ë§Œ Cloud Run ëŸ°íƒ€ì„ env_varsì€ ë¬´íš¨
   (ì´ë¯¸ ë¹Œë“œëœ HTML/JSì— í•˜ë“œì½”ë”©ë˜ì–´ ìˆê¸° ë•Œë¬¸)
```

---

## ğŸŸ¡ ë¬¸ì œì  3: API ë¼ìš°íŒ… ê²½ë¡œ ë¶ˆì¼ì¹˜

### ìœ„ì¹˜
- **ë°±ì—”ë“œ**: `backend/app/main.py` (ë¼ì¸ 209)
- **ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸**: `backend/app/api/endpoints/status.py` (ë¼ì¸ 89)

### í˜„ì¬ ì½”ë“œ
```python
# backend/app/main.py (ë¼ì¸ 209)
app.include_router(status.router, prefix="/api/v1/status", tags=["Status"])

# backend/app/api/endpoints/status.py (ë¼ì¸ 89)
@router.get("/dev/reset-all")
def reset_all_data(db: Session = Depends(get_db)):
    ...
```

### ê²½ë¡œ ê³„ì‚°
```
RouterPrefix: /api/v1/status
Endpoint Path: /dev/reset-all
ìµœì¢… ê²½ë¡œ: /api/v1/status/dev/reset-all âœ…

í•˜ì§€ë§Œ ì‚¬ìš©ìëŠ” ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œ:
fetch('/api/v1/dev/reset-all')  âŒ
ì‘ë‹µ: 404 Not Found
```

### í˜„ì¬ ë¼ìš°í„° ë“±ë¡

```python
app.include_router(auth.router, prefix="/api/v1/auth")
app.include_router(users.router, prefix="/api/v1/users")
app.include_router(status.router, prefix="/api/v1/status")  # â† reset-allì€ ì—¬ê¸°!
app.include_router(scrape.router, prefix="/api/v1/scrape")
app.include_router(analyze.router, prefix="/api/v1/analyze")
# ... 17ê°œ ë¼ìš°í„° ë“±ë¡
```

### ëª¨ë“  status ì—”ë“œí¬ì¸íŠ¸
```
GET  /api/v1/status/status              (ì‹œìŠ¤í…œ ìƒíƒœ)
GET  /api/v1/status/naver-health        (Naver API ê²€ì¦)
GET  /api/v1/status/dev/reset-all       (DB ì´ˆê¸°í™”) â† ìµœê·¼ ì¶”ê°€
POST /api/v1/status/dev/reset-all       (DB ì´ˆê¸°í™”) â† ì¶”ê°€ë¨
```

### ì˜í–¥
```javascript
// ì‚¬ìš©ì ì½”ë“œ (ì˜ëª»ë¨)
fetch('/api/v1/dev/reset-all', { method: 'GET' })
// 404 Not Found

// ì˜¬ë°”ë¥¸ ê²½ë¡œ
fetch('/api/v1/status/dev/reset-all', { method: 'GET' })
// 200 OK
```

---

## ğŸŸ¡ ë¬¸ì œì  4: UUID íƒ€ì… ê²€ì¦ ì‹¤íŒ¨

### ìœ„ì¹˜
- **ë°±ì—”ë“œ**: `backend/app/api/endpoints/analyze.py` (ë¼ì¸ 401-407)
- **í”„ë¡ íŠ¸ì—”ë“œ**: `frontend/src/lib/api.ts` (ë¼ì¸ ~506)

### í˜„ì¬ ì½”ë“œ
```python
# backend/app/api/endpoints/analyze.py
@router.get("/history/{client_id}")
def get_analysis_history(
    client_id: UUID,  # â† FastAPIê°€ ìë™ ê²€ì¦
    db: Session = Depends(get_db),
):
    ...
```

```typescript
// frontend/src/lib/api.ts
export const getAnalysisHistory = async (clientId: string): Promise<any[]> => {
    const response = await api.get(`/api/v1/analyze/history/${clientId}`);
    return response.data;
};
```

### ê²€ì¦ í”„ë¡œì„¸ìŠ¤

```
ìš”ì²­: /api/v1/analyze/history/123  (string)
â†“
FastAPI ìë™ ê²€ì¦:
  client_id: UUID â†’ "123" íŒŒì‹± ì‹œë„
  âŒ "123"ì€ ìœ íš¨í•œ UUID í˜•ì‹ì´ ì•„ë‹˜
  â†’ HTTPException(422, "value is not a valid uuid")

ì‘ë‹µ: 422 Unprocessable Entity
{
  "detail": [
    {
      "type": "uuid_parsing",
      "loc": ["path", "client_id"],
      "msg": "value is not a valid uuid"
    }
  ]
}
```

### ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤

```typescript
// SetupWizard.tsx
const [newClientId, setNewClientId] = useState<string | null>(null);

useEffect(() => {
    if (newClientId) {
        getAnalysisHistory(newClientId)  // â† newClientIdê°€ ë¬¸ìì—´
            .then(setHistory)
            .catch(err => {
                // err.response?.status === 422
                console.error('Failed:', err);
            });
    }
}, [newClientId]);

// newClientIdê°€ ë‹¤ìŒê³¼ ê°™ì„ ë•Œ ì—ëŸ¬ ë°œìƒ:
// - undefined
// - null
// - "invalid-format"
// - "12345"
// - ìœ íš¨í•œ UUIDê°€ ì•„ë‹Œ ê°’
```

### FastAPIì˜ UUID ìë™ ê²€ì¦

```python
# FastAPIê°€ path parameterì—ì„œ UUIDë¡œ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´
# ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰:

client_id: UUID  # ì •ì˜
â†“
# ìš”ì²­ ê²½ë¡œ: /api/v1/analyze/history/550e8400-e29b-41d4-a716-446655440000
client_id = uuid.UUID("550e8400-e29b-41d4-a716-446655440000")  # âœ… ì„±ê³µ

# ìš”ì²­ ê²½ë¡œ: /api/v1/analyze/history/not-a-uuid
client_id = uuid.UUID("not-a-uuid")  # âŒ ValueError
# â†’ HTTPException(422)
```

---

## ğŸ”´ ë¬¸ì œì  5: ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ê³ ì •

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 240)

### í˜„ì¬ ì½”ë“œ
```typescript
// Step 2: ìŠ¤í¬ë˜í•‘ íŠ¸ë¦¬ê±°
if (platform === 'NAVER_PLACE') {
    scrapePlace(keyword, newClientId!)
        .then(data => console.log('âœ… Place scraping triggered'))
        .catch(err => toast.warning('ìŠ¤í¬ë˜í•‘ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.'));
}

// Step 3: ê³ ì • 2ì´ˆ ëŒ€ê¸°
console.log(`â³ Waiting 2 seconds for scraping to complete...`);
setTimeout(async () => {
    try {
        const results = await getScrapeResults(newClientId!, keyword, platform);
        setScrapeResults(results);
        setShowResults(true);
    } catch (err) {
        toast.warning('ê²°ê³¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
}, 2000);  // â† í•˜ë“œì½”ë”©ëœ 2ì´ˆ!
```

### ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì‹œê°„ ë¶„ì„

```
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì†Œìš” ì‹œê°„:

ìµœìƒì˜ ê²½ìš°: ~1-2ì´ˆ
- ë„¤íŠ¸ì›Œí¬ ë¹ ë¦„
- í˜ì´ì§€ ë¡œë“œ ë¹ ë¦„
- íŒŒì‹± ê°„ë‹¨

ì¼ë°˜ì ì¸ ê²½ìš°: 5-10ì´ˆ
- ë„¤íŠ¸ì›Œí¬ ì¼ë°˜
- í˜ì´ì§€ JavaScript ë Œë”ë§ í•„ìš”
- BeautifulSoup íŒŒì‹±

ìµœì•…ì˜ ê²½ìš°: 20-30ì´ˆ+
- ë„¤íŠ¸ì›Œí¬ ëŠë¦¼
- CDN ìºì‹œ ë¯¸ìŠ¤
- Selenium íƒ€ì„ì•„ì›ƒ
- ì„œë²„ ì‘ë‹µ ì§€ì—°
- Cloudflare ê²€ì¦

ë°°í¬ í™˜ê²½:
Cloud Run (us-west1) â†’ Naver ì„œë²„ (í•œêµ­)
ì§€ì—°: +200-500ms ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ
ì´ ì‹œê°„: 2ì´ˆ + 200-500ms = 2.2-2.5ì´ˆ (ë¶ˆì¶©ë¶„!)
```

### ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤

```
ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ ë„¤íŠ¸ì›Œí¬
0ms: scrapePlace() í˜¸ì¶œ â†’ ì¦‰ì‹œ ì‘ë‹µ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ë§Œ ë“±ë¡)
2000ms: getScrapeResults() í˜¸ì¶œ
  â†’ ìŠ¤í¬ë˜í•‘ ì•„ì§ ì§„í–‰ ì¤‘ (1ì´ˆë§Œ ê²½ê³¼)
  â†’ "ë°ì´í„° ì—†ìŒ" í‘œì‹œ âŒ
  â†’ ì‚¬ìš©ì: "ì™œ ë°ì´í„°ê°€ ì•ˆ ë‚˜ì™€?" ğŸ¤”

ì‹œë‚˜ë¦¬ì˜¤ 2: ëŠë¦° ë„¤íŠ¸ì›Œí¬
0ms: scrapePlace() í˜¸ì¶œ
2000ms: getScrapeResults() í˜¸ì¶œ
  â†’ ìŠ¤í¬ë˜í•‘ ì•„ì§ ì§„í–‰ ì¤‘ (4-5ì´ˆ ë‚¨ìŒ)
  â†’ "ë°ì´í„° ì—†ìŒ" í‘œì‹œ
  â†’ ì‹¤ì œë¡œëŠ” 2ì´ˆ í›„ ë°ì´í„° ë“¤ì–´ì˜´ ğŸ˜¤
  â†’ ì‚¬ìš©ìê°€ ìƒˆë¡œê³ ì¹¨í•´ì•¼ í•¨

ì‹œë‚˜ë¦¬ì˜¤ 3: ìµœì•…ì˜ ê²½ìš°
0ms: scrapePlace() í˜¸ì¶œ
2000ms: getScrapeResults() í˜¸ì¶œ
  â†’ ìŠ¤í¬ë˜í•‘ ì¤‘... (15ì´ˆ ë” í•„ìš”)
  â†’ "ë°ì´í„° ì—†ìŒ"
  â†’ ì‚¬ìš©ì: "ì´ê²Œ ë­ì•¼, ì‘ë™í•˜ì§€ ì•Šë„¤" ğŸ’¢
```

### ê·¼ë³¸ ì›ì¸

```
BackgroundTasksì˜ íŠ¹ì„±:
1. add_task()ëŠ” ìš”ì²­ì´ ëë‚  ë•Œ ì‹¤í–‰
2. ì‘ë‹µì€ ì¦‰ì‹œ ë°˜í™˜
3. ì‹¤ì œ ì‘ì—…ì€ ì‘ë‹µ í›„ ì‹œì‘

íƒ€ì´ë°:
Request â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”œâ”€ 0ms: scrapePlace() í˜¸ì¶œ
  â”œâ”€ 5ms: ì‘ë‹µ ë°˜í™˜
  â””â”€ 5ms: BackgroundTask ì‹¤í–‰ ì‹œì‘
  
Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”œâ”€ 5ms: í”„ë¡ íŠ¸ì—”ë“œê°€ ì‘ë‹µ ë°›ìŒ
  â”œâ”€ 2000ms: setTimeout ë§Œë£Œ
  â”œâ”€ 2005ms: getScrapeResults() í˜¸ì¶œ
  â””â”€ ìŠ¤í¬ë˜í•‘ì€ ì•„ì§ 5ì´ˆë§Œ ê²½ê³¼ âŒ
```

---

## ğŸ”´ ë¬¸ì œì  6: ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì—ëŸ¬ ë¬´ì‹œ

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 226-254)

### í˜„ì¬ ì½”ë“œ
```typescript
if (platform === 'NAVER_PLACE') {
    scrapePlace(keyword, newClientId!)
        .then((data) => {
            console.log('âœ… [Step 2-A] Place scraping triggered');
            console.log('   Response:', data);
        })
        .catch((err) => {
            console.error('âš ï¸ [Step 2-A] Place scraping failed:', err);
            // âŒ ì—ëŸ¬ë¥¼ ê¸°ë¡í•˜ì§€ë§Œ ë¬´ì‹œ!
            toast.warning('ìŠ¤í¬ë˜í•‘ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.');
            // âŒ catch ì´í›„ ê³„ì† ì§„í–‰!
        });
}

// âŒ ë°”ë¡œ ë‹¤ìŒ ì½”ë“œ ì‹¤í–‰ (ì—ëŸ¬ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
toast.info('ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...');

setTimeout(async () => {
    // âŒ scrapePlaceê°€ ì„±ê³µí–ˆëŠ”ì§€ ì‹¤íŒ¨í–ˆëŠ”ì§€ ëª¨ë¥¸ ìƒíƒœë¡œ ê²°ê³¼ ì¡°íšŒ
    const results = await getScrapeResults(newClientId!, keyword, platform);
    if (results.has_data && results.results.length > 0) {
        setScrapeResults(results);
        setShowResults(true);
        toast.success('ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!');
    } else {
        setScrapeResults(results);
        setShowResults(true);
        toast.info('ì¡°ì‚¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‚˜íƒ€ë‚  ì˜ˆì •ì…ë‹ˆë‹¤.');
    }
}, 2000);
```

### ë¬¸ì œ ë¶„ì„

```
ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ë•Œ:
1ï¸âƒ£ scrapePlace() ì‹¤íŒ¨
   â””â”€ catch() í˜¸ì¶œ: console.error() + toast.warning()
   â””â”€ âŒ í•˜ì§€ë§Œ ë³€ìˆ˜ë‚˜ í”Œë˜ê·¸ë¡œ ì €ì¥ ì•ˆ í•¨

2ï¸âƒ£ ì´í›„ ì½”ë“œëŠ” ì—¬ì „íˆ ì‹¤í–‰ë¨
   â””â”€ "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤" í† ìŠ¤íŠ¸ í‘œì‹œ (ê±°ì§“!)
   â””â”€ 2ì´ˆ ëŒ€ê¸°

3ï¸âƒ£ getScrapeResults() í˜¸ì¶œ
   â””â”€ ìŠ¤í¬ë˜í•‘ì´ ì‹¤íŒ¨í–ˆìœ¼ë¯€ë¡œ ê²°ê³¼ê°€ ì—†ìŒ
   â””â”€ has_data: false
   â””â”€ "ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‚˜íƒ€ë‚  ì˜ˆì •" (ì ˆëŒ€ ì•ˆ ë‚˜íƒ€ë‚¨)

4ï¸âƒ£ ì‚¬ìš©ì ê²½í—˜
   âŒ "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤" â†’ ì‹¤íŒ¨í–ˆëŠ”ë° ì§„í–‰ ì¤‘ì´ë¼ê³  ìƒê°í•¨
   âŒ "ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‚˜íƒ€ë‚  ì˜ˆì •" â†’ ê³„ì† ëŒ€ê¸°í•˜ë‹¤ í¬ê¸°
   âŒ ì‹¤ì œ ì—ëŸ¬(ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ, Selenium ì˜¤ë¥˜ ë“±)ë¥¼ ëª¨ë¦„
```

### ë°±ì—”ë“œ ì—ëŸ¬ ì˜ˆì‹œ

```python
# backend/app/worker/tasks.py
def execute_place_sync(keyword: str, client_id_str: str = None):
    try:
        results = asyncio.run(run_place_scraper(keyword))
    except Exception as e:
        logger.error(f"Scraping failed for {keyword}: {e}")
        # âŒ ë°±ì—”ë“œëŠ” ë¡œê·¸í•˜ì§€ë§Œ í”„ë¡ íŠ¸ì—”ë“œì— ì•Œë¦¬ì§€ ì•ŠìŒ
        error_msg = str(e)
        results = []  # ë¹ˆ ê²°ê³¼ ë°˜í™˜
    
    # ë¹ˆ ê²°ê³¼ê°€ DBì— ì €ì¥ë¨
    # í”„ë¡ íŠ¸ì—”ë“œëŠ” "dataê°€ ì—†ë‹¤"ê³ ë§Œ ì•Œê²Œ ë¨
```

### ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤

```
ê°€ëŠ¥í•œ ì—ëŸ¬ë“¤:
1. Selenium íƒ€ì„ì•„ì›ƒ
2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨
3. Cloudflare ì°¨ë‹¨
4. ë©”ëª¨ë¦¬ ë¶€ì¡± (Cloud Run)
5. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨

ëª¨ë‘ ë™ì¼í•œ ê²°ê³¼:
has_data: false, results: []
â†“
ì‚¬ìš©ìëŠ” "ë°ì´í„°ê°€ ì—†ë‹¤"ê³ ë§Œ ì¸ì§€
(ì‹¤ì œë¡œëŠ” ì—ëŸ¬ ë°œìƒ)
```

---

## ğŸ”´ ë¬¸ì œì  7: ì¸ì¦ ë¯¸ê²€ì¦ ìŠ¤í¬ë˜í•‘ ì—”ë“œí¬ì¸íŠ¸

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/api/endpoints/scrape.py` (ë¼ì¸ 11-54)

### í˜„ì¬ ì½”ë“œ
```python
@router.post("/place", response_model=ScrapeResponse)
def trigger_place_scrape(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
    # âŒ get_current_user ì—†ìŒ!
):
    task_id = str(uuid.uuid4())
    background_tasks.add_task(scrape_place_task, request.keyword, request.client_id)
    return ScrapeResponse(...)

@router.post("/view", response_model=ScrapeResponse)
def trigger_view_scrape(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
    # âŒ ì¸ì¦ ì—†ìŒ!
):
    ...

@router.post("/ad", response_model=ScrapeResponse)
def trigger_ad_scrape(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
    # âŒ ì¸ì¦ ì—†ìŒ!
):
    ...
```

### ë‹¤ë¥¸ ì—”ë“œí¬ì¸íŠ¸ì™€ì˜ ë¹„êµ

```python
# âœ… ì¸ì¦ì´ ìˆëŠ” ì—”ë“œí¬ì¸íŠ¸
@router.get("/history/{client_id}")
def get_analysis_history(
    client_id: UUID,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)  # âœ… ì¸ì¦ í•„ìš”
):
    ...

# âŒ ì¸ì¦ì´ ì—†ëŠ” ì—”ë“œí¬ì¸íŠ¸
@router.post("/place")
def trigger_place_scrape(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)  # âŒ ì¸ì¦ ì—†ìŒ!
):
    ...
```

### ë³´ì•ˆ ìœ„í—˜

```
ê³µê²© ì‹œë‚˜ë¦¬ì˜¤:

1ï¸âƒ£ ê³µê²©ìê°€ API ë°œê²¬
   curl -X POST https://dentalanal.app/api/v1/scrape/place \
     -H "Content-Type: application/json" \
     -d '{"keyword": "ì„í”Œë€íŠ¸", "client_id": "any-uuid"}'

2ï¸âƒ£ ì¸ì¦ ê²€ì¦ ì—†ìŒ â†’ ì„±ê³µ (202 Accepted)

3ï¸âƒ£ ë°˜ë³µ ìŠ¤í¬ë˜í•‘
   for i in range(1000):
       trigger_place_scrape("keyword", "client_id")
   
   ê²°ê³¼:
   - í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ (CPU, ë©”ëª¨ë¦¬, ë„¤íŠ¸ì›Œí¬)
   - íƒ€ ì‚¬ìš©ì ì„±ëŠ¥ ì €í•˜
   - ë„¤ì´ë²„ IP ì°¨ë‹¨ ìœ„í—˜
   - ë¹„ìš© ì¦ê°€

4ï¸âƒ£ ì„ì˜ í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° ì¡°íšŒ
   {"client_id": "victim-client-uuid", "keyword": "..."}
   â†’ í”¼í•´ì í´ë¼ì´ì–¸íŠ¸ì— ëŒ€í•´ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
   â†’ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
```

### ì„¤ê³„ ì˜ë„ ë¶„ì„

```python
# main.py ë¼ì¸ 183
app.include_router(scrape.router, prefix="/api/v1/scrape", ...)

# app.py ë¼ì¸ 177 (CloudFunctions ì‹œì ˆ ì˜ˆìƒ)
# Cloud Schedulerì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•´ ì¸ì¦ ì—†ì´ ì„¤ê³„?
# â†’ í•˜ì§€ë§Œ Cloud Runì—ì„œëŠ” ìì²´ ì¸ì¦(IAM) ìˆìŒ
# â†’ ê³µê°œ URLì¸ --allow-unauthenticated ë•Œë¬¸ì— ì¸ì¦ í•„ìˆ˜
```

---

## ğŸ”´ ë¬¸ì œì  8: BackgroundTasks ì‹ ë¢°ì„± ë¶€ì¡±

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/api/endpoints/scrape.py` (ë¼ì¸ 17-18)
- **íŒŒì¼**: `backend/app/worker/tasks.py` (ë¼ì¸ 33-100)

### í˜„ì¬ ì•„í‚¤í…ì²˜

```
ìš”ì²­
  â†“
Flask/FastAPI í•¸ë“¤ëŸ¬
  â”œâ”€ BackgroundTasks.add_task()  â† ì‘ì—… ë“±ë¡
  â””â”€ ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜ (202 Accepted)
  
ì‘ë‹µ ë°˜ë£Œ í›„
  â†“
BackgroundTask ì‹¤í–‰
  â”œâ”€ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰
  â”œâ”€ ë°ì´í„° ì €ì¥
  â””â”€ ì•Œë¦¼ ë°œì†¡
  
ë§Œì•½ ì‹¤íŒ¨?
  â”œâ”€ ë¡œê¹…ë§Œ í•¨
  â”œâ”€ ì¬ì‹œë„ ì—†ìŒ
  â””â”€ ì‚¬ìš©ì ëª¨ë¦„
```

### ë¬¸ì œì 

```python
# backend/app/worker/tasks.py (ë¼ì¸ 33-53)
def execute_place_sync(keyword: str, client_id_str: str = None):
    try:
        results = asyncio.run(run_place_scraper(keyword))
    except Exception as e:
        logger.error(f"Scraping failed: {e}")  # â† ë¡œê·¸ë§Œ í•¨
        results = []  # â† ë¹ˆ ê²°ê³¼ë¡œ ê³„ì† ì§„í–‰
    
    db = SessionLocal()
    try:
        if results:
            service.save_place_results(...)  # â† DB ì €ì¥ ì‹œë„
        # ...
    except Exception as e:
        logger.error(f"Saving failed: {e}")  # â† ë‹¤ì‹œ ì‹¤íŒ¨í•˜ë©´ ë¡œê·¸ë§Œ í•¨
    finally:
        db.close()
    
    return results  # â† returnì€ íš¨ê³¼ ì—†ìŒ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì´ë¯€ë¡œ)
```

### ì‹ ë¢°ì„± ë¬¸ì œ

```
ì‹œë‚˜ë¦¬ì˜¤: ìŠ¤í¬ë˜í•‘ ì¤‘ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ëŠê¹€

0ms: add_task(execute_place_sync, keyword, client_id)
5ms: ì‘ë‹µ ë°˜í™˜ (ì‚¬ìš©ìëŠ” "ì„±ê³µ"ì´ë¼ê³  ìƒê°í•¨)
2000ms: BackgroundTask ì‹¤í–‰
  â”œâ”€ ìŠ¤í¬ë˜í•‘ ì„±ê³µ
  â”œâ”€ SessionLocal() ìƒì„±
  â”œâ”€ service.save_place_results() í˜¸ì¶œ
  â””â”€ âŒ DB ì—°ê²° ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ)
  
ê²°ê³¼:
  â”œâ”€ logger.error() ë¡œê·¸ë§Œ ë‚¨ìŒ
  â”œâ”€ DBì— ë°ì´í„° ì €ì¥ ì•ˆ ë¨
  â”œâ”€ ì‚¬ìš©ìëŠ” ëª¨ë¦„ (ì´ë¯¸ ì‘ë‹µë°›ì•˜ìœ¼ë¯€ë¡œ)
  â””â”€ 2ì´ˆ í›„: getScrapeResults() â†’ ë¹ˆ ê²°ê³¼
  
ì‚¬ìš©ì: "ì•„ë¬´ ë°ì´í„°ë„ ì—†ë‹¤ê³ ??" ğŸ˜¤
ê°œë°œì: "ë¡œê·¸ë¥¼ ë´¤ë‚˜?" (ì‚¬ìš©ìëŠ” ë¡œê·¸ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ)
```

### BackgroundTasksì˜ í•œê³„

```
ë³´ì¥ì‚¬í•­:
âŒ ì‘ì—…ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ì‘ë‹µì„ ì§€ì—°í•˜ì§€ ì•ŠìŒ
âŒ ì‘ì—… ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ì•ˆ í•¨
âŒ ì‘ì—… ìƒíƒœë¥¼ ì¶”ì í•˜ì§€ ì•ŠìŒ
âŒ ì‘ì—… ì†ì‹¤ ê°€ëŠ¥ (í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ ì‹œ)

ë” ë‚˜ì€ ëŒ€ì•ˆ:
âœ… Celery (ì‘ì—… í, ì¬ì‹œë„, ëª¨ë‹ˆí„°ë§)
âœ… Cloud Tasks (GCP ê´€ë¦¬í˜•, ì¬ì‹œë„)
âœ… Redis Queue (ê°€ë²¼ìš´ ì‘ì—… í)
âœ… APScheduler (ì£¼ê¸°ì  ì‘ì—…)

í˜„ì¬: BackgroundTasks (ìµœì†Œí•œì˜ ê¸°ëŠ¥ë§Œ ì œê³µ)
```

---

## ğŸŸ  ë¬¸ì œì  9: ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¶”ì  ë¶ˆê°€

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/api/endpoints/scrape.py` (ë¼ì¸ 17)
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 231)

### í˜„ì¬ ì½”ë“œ
```python
# backend/app/api/endpoints/scrape.py
def trigger_place_scrape(...):
    task_id = str(uuid.uuid4())  # â† UUIDë§Œ ìƒì„±í•˜ê³  ì‚¬ìš© ì•ˆ í•¨!
    background_tasks.add_task(scrape_place_task, keyword, client_id)
    
    return ScrapeResponse(
        task_id=task_id,  # â† í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜
        message="..."
    )
```

```typescript
// frontend/src/components/setup/SetupWizard.tsx
scrapePlace(keyword, newClientId!)
    .then((data) => {
        console.log('âœ… Response:', data);  // â† task_id ë°›ì§€ë§Œ ë¯¸ì‚¬ìš©
        // task_idë¥¼ ì‚¬ìš©í•˜ëŠ” ì½”ë“œê°€ ì—†ìŒ
    })
```

### ë¬¸ì œì 

```
task_idë¥¼ ë°›ì§€ë§Œ í™œìš© ë¶ˆê°€:

âŒ ì‘ì—… ìƒíƒœ ì¡°íšŒ ë¶ˆê°€
   // ì‚¬ìš©ì: "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆë‚˜?"
   // API ì—†ìŒ: /api/v1/scrape/status/{task_id}

âŒ ì‘ì—… ì·¨ì†Œ ë¶ˆê°€
   // ì‚¬ìš©ì: "ìŠ¤í¬ë˜í•‘ ë©ˆì¶°"
   // API ì—†ìŒ: /api/v1/scrape/cancel/{task_id}

âŒ ì‘ì—… ì§„í–‰ë¥  ë¯¸ì œê³µ
   // ì‚¬ìš©ì: "ì–¼ë§ˆë‚˜ ë‚¨ì•˜ì§€?"
   // í”¼ë“œë°± ì—†ìŒ

âŒ ì‘ì—… ì¬ì‹œë„ ë¶ˆê°€
   // ì‘ì—… ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ì‹œ í˜¸ì¶œí•´ì•¼ í•¨
```

### UX ë¬¸ì œ

```
ì‚¬ìš©ì ê´€ì :

1. "ì¡°ì‚¬ ì‹œì‘" í´ë¦­
2. "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤" í† ìŠ¤íŠ¸ (ì¦‰ì‹œ ì‚¬ë¼ì§)
3. ??? (ë¬´ì—‡ì´ ì¼ì–´ë‚˜ê³  ìˆëŠ”ê°€?)
4. 2ì´ˆ ëŒ€ê¸°
5. "ë°ì´í„° ì—†ìŒ" ë˜ëŠ” "ë°ì´í„° í‘œì‹œ"
6. ì‚¬ìš©ì: "ì„±ê³µí–ˆë‚˜? ì‹¤íŒ¨í–ˆë‚˜?" ğŸ¤·
```

---

## ğŸŸ  ë¬¸ì œì  10: í´ë¼ì´ì–¸íŠ¸ ID ê²€ì¦ ì§€ì—°

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 180-195)

### í˜„ì¬ ì½”ë“œ
```typescript
const handleNext = async () => {
    if (currentStep === 1) {
        if (!clientName) { 
            toast.error('ì—…ì²´ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'); 
            return; 
        }
        // âŒ ì—…ì²´ ìƒì„± ì „ì— í´ë¼ì´ì–¸íŠ¸ ID ê²€ì¦ ì•ˆ í•¨
        try {
            const existing = clientSuggestions.find(c => c.name === clientName);
            if (existing) {
                setNewClientId(existing.id);  // â† íƒ€ì… ê²€ì¦ ì—†ìŒ
            } else {
                const created = await createClient({
                    name: clientName,
                    industry,
                    agency_id: user?.agency_id || '...'
                });
                setNewClientId(created.id);  // â† íƒ€ì… ê²€ì¦ ì—†ìŒ
            }
            setCurrentStep(2);
        } catch {
            toast.error('ì—…ì²´ ë“±ë¡ ì¤‘ ì˜¤ë¥˜');
        }
    } else if (currentStep === 2) {
        // ...
    } else if (currentStep === 3) {
        // ì—¬ê¸°ì„œ ì²˜ìŒìœ¼ë¡œ newClientIdê°€ ì‚¬ìš©ë¨
        const results = await getScrapeResults(newClientId!, keyword, platform);
        // â† ë§Œì•½ newClientIdê°€ ì˜ëª»ëœ í˜•ì‹ì´ë©´ 422 ì—ëŸ¬
    }
};
```

### ë¬¸ì œì 

```
ê²€ì¦ íƒ€ì´ë° ë¬¸ì œ:

Step 1
  â”œâ”€ clientName ì…ë ¥
  â””â”€ newClientId ì„¤ì • (ê²€ì¦ ì•ˆ í•¨)

Step 2
  â”œâ”€ íƒ€ê²Ÿ ì…ë ¥
  â””â”€ ...

Step 3
  â”œâ”€ âŒ ì²˜ìŒ ì‚¬ìš©: getScrapeResults(newClientId, ...)
  â”œâ”€ ì—ëŸ¬: 422 Unprocessable Entity
  â””â”€ ì‚¬ìš©ì: "ì–´? ë­ê°€ ë¬¸ì œì•¼?"

ì—ëŸ¬ê°€ Step 1ì—ì„œ ê°ì§€ë˜ì—ˆì„ ìˆ˜ë„ ìˆì—ˆëŠ”ë°
Step 3ê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ í•¨!
```

### ê°œì„  ì „ëµ

```typescript
// Step 1ì—ì„œ ì¦‰ì‹œ ê²€ì¦
const created = await createClient({...});

// ì‘ë‹µ ê²€ì¦
if (!created.id || !isValidUUID(created.id)) {
    toast.error('ì—…ì²´ ë“±ë¡ í›„ ID ìˆ˜ì‹  ì‹¤íŒ¨');
    return;  // â† Step 2 ì§„í–‰ ë°©ì§€
}

setNewClientId(created.id);
```

---

---

## ğŸ”´ ë¬¸ì œì  11: rank_change í•„ë“œ ë¯¸êµ¬í˜„ (ë°ì´í„° ë¬´ê²°ì„±)

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/models/models.py` (ë¼ì¸ 243)
- **íŒŒì¼**: `frontend/src/components/setup/ScrapeResultsDisplay.tsx` (ë¼ì¸ 14)

### í˜„ì¬ ì½”ë“œ
```python
# backend/app/models/models.py
class DailyRank(Base):
    __tablename__ = "daily_ranks"
    id = Column(GUID, primary_key=True, default=uuid.uuid4)
    rank = Column(Integer, nullable=False)
    captured_at = Column(DateTime(timezone=True), server_default=func.now())
    # âŒ rank_change í•„ë“œê°€ ì—†ìŒ!
```

```typescript
// frontend/src/components/setup/ScrapeResultsDisplay.tsx (ë¼ì¸ 8-10)
interface ScrapeResult {
    rank: number;
    rank_change?: number;  // â† APIì—ì„œ ë°˜í™˜ë˜ì§€ ì•ŠìŒ!
    target_name: string;
    ...
}
```

```python
# backend/app/api/endpoints/analyze.py (ë¼ì¸ 933-936)
result_item = {
    "rank": r.rank,
    "rank_change": r.rank_change,  # â† AttributeError: 'DailyRank' has no attribute 'rank_change'
    ...
}
```

### ë¬¸ì œì 
```
ì‹¤ì œ íë¦„:
1ï¸âƒ£ getScrapeResults() í˜¸ì¶œ
   â””â”€ backend: GET /api/v1/analyze/scrape-results/{client_id}
2ï¸âƒ£ backendì—ì„œ DailyRank ê°ì²´ ì¡°íšŒ
3ï¸âƒ£ response êµ¬ì„±:
   result_item = {
       "rank": r.rank,
       "rank_change": r.rank_change,  # âŒ AttributeError!
       ...
   }
4ï¸âƒ£ 500 Internal Server Error ë°˜í™˜
5ï¸âƒ£ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì˜ˆì™¸ ë°œìƒ

ê²°ê³¼:
  "ì¡°ì‚¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" (ì‹¤ì œëŠ” ë°±ì—”ë“œ ì—ëŸ¬)
```

### ì¦‰ì‹œ í˜„ìƒ
```
ìš”ì²­: GET /api/v1/analyze/scrape-results/uuid
ì‘ë‹µ: 500 Internal Server Error
{
  "detail": "'DailyRank' object has no attribute 'rank_change'"
}

ì½˜ì†”: 
AttributeError: 'DailyRank' object has no attribute 'rank_change'
File "backend/app/api/endpoints/analyze.py", line 936, in get_scrape_results
    "rank_change": r.rank_change,
```

### í•´ê²° ë°©ë²•
```python
# 1. ëª¨ë¸ì— í•„ë“œ ì¶”ê°€
class DailyRank(Base):
    rank = Column(Integer, nullable=False)
    rank_change = Column(Integer, nullable=True, default=0)
    captured_at = Column(DateTime(timezone=True), ...)

# 2. ë˜ëŠ” APIì—ì„œ í•„ë“œ ì œê±°
result_item = {
    "rank": r.rank,
    # "rank_change": r.rank_change,  # ì œê±°
    "target_name": ...
}

# 3. ë˜ëŠ” rank_change ê³„ì‚° ë¡œì§ ì¶”ê°€
# ì´ì „ rankì™€ í˜„ì¬ rankì˜ ì°¨ì´ ê³„ì‚°
```

---

## ğŸ”´ ë¬¸ì œì  12: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ëˆ„ìˆ˜ (ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜)

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/worker/tasks.py` (ë¼ì¸ 33-127)

### í˜„ì¬ ì½”ë“œ
```python
def execute_place_sync(keyword: str, client_id_str: str = None):
    db = SessionLocal()  # â† ì„¸ì…˜ ìƒì„±
    try:
        service = AnalysisService(db)
        if results:
            service.save_place_results(keyword, results, client_uuid)
        
        admins = db.query(User).filter(...).all()
        for admin in admins:
            note = Notification(...)
            db.add(note)
        db.commit()  # â† DB ì»¤ë°‹
        
    except Exception as e:
        logger.error(f"Saving failed: {e}")
        # âŒ db.rollback() ì—†ìŒ!
    finally:
        db.close()  # â† ì„¸ì…˜ ë‹«ê¸°
    
    return results
```

### ë¬¸ì œì 

```
ì‹œë‚˜ë¦¬ì˜¤: ë°ì´í„° ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ

1ï¸âƒ£ service.save_place_results() í˜¸ì¶œ
2ï¸âƒ£ db.add() ë° db.commit()
3ï¸âƒ£ âŒ db.flush() ì¤‘ ì—ëŸ¬ ë°œìƒ (FK ì œì•½ì¡°ê±´, íŠ¸ë¦¬ê±° ë“±)
4ï¸âƒ£ except ë¸”ë¡ìœ¼ë¡œ ì´ë™
   â””â”€ logger.error() ë¡œê·¸ë§Œ í•¨
   â””â”€ âŒ db.rollback() ì—†ìŒ!
5ï¸âƒ£ finally ë¸”ë¡ ì‹¤í–‰
   â””â”€ db.close()

ê²°ê³¼:
  - ë¶€ë¶„ì ìœ¼ë¡œ ì €ì¥ëœ ë°ì´í„° ë‚¨ìŒ (ì¼ê´€ì„± ê¹¨ì§)
  - ì„¸ì…˜ì€ ë‹«í˜”ì§€ë§Œ íŠ¸ëœì­ì…˜ ë¯¸ë¡¤ë°±
  - ë‹¤ìŒ ì„¸ì…˜ì—ì„œ ì¥ì•  ë°œìƒ ê°€ëŠ¥
  - ë°ì´í„°ë² ì´ìŠ¤ ë½(lock) ë°œìƒ ê°€ëŠ¥
```

### ë” ì‹¬ê°í•œ ë¬¸ì œ

```python
# Notification ì €ì¥ ì‹œ ì—ëŸ¬ ë°œìƒ ì˜ˆì‹œ
for admin in admins:  # â† admins ì¿¼ë¦¬ ì„±ê³µ
    note = Notification(
        user_id=admin.id,  # â† adminì´ ì‚­ì œë˜ì—ˆë‹¤ë©´?
        ...
    )
    db.add(note)

db.commit()  # âŒ FK ì œì•½ì¡°ê±´ ìœ„ë°˜
# â†’ except ë¸”ë¡ìœ¼ë¡œ ì´ë™
# â†’ db.rollback() ì—†ìŒ
# â†’ ì´ì „ì˜ save_place_results()ì—ì„œ ì¶”ê°€í•œ DailyRankëŠ” ë‚¨ìŒ!
# â†’ ë°ì´í„° ë¶ˆì¼ì¹˜!
```

### í•´ê²° ë°©ë²•
```python
def execute_place_sync(keyword: str, client_id_str: str = None):
    db = SessionLocal()
    try:
        service = AnalysisService(db)
        if results:
            service.save_place_results(keyword, results, client_uuid)
        
        # ... Notification ì¶”ê°€ ...
        
        db.commit()
    except Exception as e:
        db.rollback()  # âœ… íŠ¸ëœì­ì…˜ ë¡¤ë°±
        logger.error(f"Failed: {e}")
    finally:
        db.close()
```

---

## ğŸŸ¡ ë¬¸ì œì  13: í”„ë¡ íŠ¸ì—”ë“œ toast ë©”ì‹œì§€ ì¼ê´€ì„± ë¶€ì¡±

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 219-270)

### í˜„ì¬ ì½”ë“œ
```typescript
// Step 1: ë¶„ì„ ì´ë ¥ ì €ì¥
const historyResponse = await saveAnalysisHistory({
    client_id: newClientId!,
    keyword,
    platform
});
console.log('âœ… Analysis history saved:', historyResponse);

// Step 2: ìŠ¤í¬ë˜í•‘ íŠ¸ë¦¬ê±°
toast.info('ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...');

if (platform === 'NAVER_PLACE') {
    scrapePlace(keyword, newClientId!)
        .then(() => {
            console.log('âœ… Place scraping triggered');
        })
        .catch((err) => {
            console.error('âš ï¸ Place scraping failed:', err);
            // âŒ ì—ëŸ¬ê°€ ë°œìƒí–ˆëŠ”ë°ë„ "ê²°ê³¼ë¥¼ ìˆ˜ì§‘ ì¤‘"ì´ë¼ê³  í‘œì‹œë¨!
        });
}

// Step 3: 2ì´ˆ ëŒ€ê¸°
setTimeout(async () => {
    const results = await getScrapeResults(newClientId!, keyword, platform);
    
    if (results.has_data && results.results.length > 0) {
        setScrapeResults(results);
        setShowResults(true);
        toast.success('ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!');  // â† í•­ìƒ ì„±ê³µìœ¼ë¡œ í‘œì‹œ
    } else {
        setScrapeResults(results);
        setShowResults(true);
        // âŒ ì•„ë¬´ í† ìŠ¤íŠ¸ë„ í‘œì‹œ ì•ˆ í•¨ (ì‚¬ìš©ìëŠ” ìƒíƒœë¥¼ ëª¨ë¦„)
        toast.info('ì¡°ì‚¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‚˜íƒ€ë‚  ì˜ˆì •ì…ë‹ˆë‹¤.');
    }
}, 2000);
```

### ë¬¸ì œì 

```
ì‚¬ìš©ì ê²½í—˜:

âœ… "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤"
â³ 2ì´ˆ ëŒ€ê¸°
ğŸ“Š ê²°ê³¼ í‘œì‹œ
âœ… "ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!" (ë°ì´í„° ì—†ì–´ë„)

ë˜ëŠ”

âœ… "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤"
â³ 2ì´ˆ ëŒ€ê¸°
âŒ ì—ëŸ¬: "Failed to fetch scrape results"
ğŸ˜• ì‚¬ìš©ì: "ë­ê°€ ë¬¸ì œì•¼?"
```

### ë¯¸ìŠ¤ë§¤ì¹­ ë©”ì‹œì§€

```
ì‹¤ì œ ìƒí™© vs ë©”ì‹œì§€:

1. ìŠ¤í¬ë˜í•‘ ì¤‘ ì—ëŸ¬
   ë©”ì‹œì§€: "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."
   í˜„ì‹¤: âŒ ìŠ¤í¬ë˜í•‘ì´ ì‹¤íŒ¨í•¨

2. ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨
   ë©”ì‹œì§€: "ì¡°ì‚¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‚˜íƒ€ë‚  ì˜ˆì •ì…ë‹ˆë‹¤."
   í˜„ì‹¤: âŒ ê²°ê³¼ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŒ (API ì—ëŸ¬)

3. ë„¤íŠ¸ì›Œí¬ ì§€ì—°
   ë©”ì‹œì§€: "ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
   í˜„ì‹¤: â³ ì•„ì§ ìŠ¤í¬ë˜í•‘ ì¤‘
```

---

## ğŸŸ¡ ë¬¸ì œì  14: ë™ì‹œ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ì²˜ë¦¬ ë¯¸í¡

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/api/endpoints/scrape.py` (ë¼ì¸ 11-54)
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 230-260)

### í˜„ì¬ ì½”ë“œ
```python
# backend/app/api/endpoints/scrape.py
@router.post("/place")
def trigger_place_scrape(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    task_id = str(uuid.uuid4())
    # âŒ ê°™ì€ keyword + client_idë¡œ ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸ ì•ˆ í•¨!
    background_tasks.add_task(scrape_place_task, request.keyword, request.client_id)
    
    return ScrapeResponse(task_id=task_id, message="...")
```

```typescript
// frontend/src/components/setup/SetupWizard.tsx
// âŒ ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ ì—°íƒ€í•˜ë©´?
scrapePlace(keyword, newClientId!)
scrapePlace(keyword, newClientId!)
scrapePlace(keyword, newClientId!)
// 3ê°œì˜ ë™ì¼í•œ ì‘ì—…ì´ ë™ì‹œì— ì‹¤í–‰ë¨!
```

### ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤

```
ì‚¬ìš©ìê°€ "ì¡°ì‚¬ ì‹œì‘" ë²„íŠ¼ì„ 3ë²ˆ ë¹ ë¥´ê²Œ í´ë¦­:

0ms: scrapePlace() í˜¸ì¶œ #1
5ms: scrapePlace() í˜¸ì¶œ #2
10ms: scrapePlace() í˜¸ì¶œ #3

ë°±ê·¸ë¼ìš´ë“œì—ì„œ:
Task #1: ìŠ¤í¬ë˜í•‘ ì‹œì‘ â†’ DBì— ì €ì¥
Task #2: ë™ì¼í•œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ â†’ ë™ì¼í•œ ë°ì´í„° ë‹¤ì‹œ ì €ì¥
Task #3: ë˜ ë‹¤ì‹œ... (ì¤‘ë³µ ë°ì´í„° 3ë°°!)

ê²°ê³¼:
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ 3ë°° ì‚¬ìš©
- CPU 3ë°° ì‚¬ìš©
- ë°ì´í„°ë² ì´ìŠ¤ì— ì¤‘ë³µ ë ˆì½”ë“œ 3ê°œ ì €ì¥
- ë¹„ìš© ì¦ê°€
- Naver IP ì°¨ë‹¨ ìœ„í—˜
```

### ë°ì´í„°ë² ì´ìŠ¤ ì¤‘ë³µ ë¬¸ì œ

```sql
-- ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í›„
SELECT * FROM daily_ranks 
WHERE client_id = 'UUID' 
AND keyword_id = 'UUID'
AND platform = 'NAVER_PLACE';

ê²°ê³¼:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id       â”‚ target_id â”‚ rank â”‚ date  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ uuid-1   â”‚ target-1  â”‚ 1    â”‚ 2/20  â”‚ â† ì²« ë²ˆì§¸ ìŠ¤í¬ë˜í•‘
â”‚ uuid-2   â”‚ target-1  â”‚ 1    â”‚ 2/20  â”‚ â† ì¤‘ë³µ! (ë‘ ë²ˆì§¸)
â”‚ uuid-3   â”‚ target-1  â”‚ 1    â”‚ 2/20  â”‚ â† ì¤‘ë³µ! (ì„¸ ë²ˆì§¸)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ë°ì´í„° ë¶„ì„ì—ì„œ:
SELECT AVG(rank) FROM daily_ranks ...
â†’ ì •í™•í•˜ì§€ ì•Šì€ í†µê³„ (ì¤‘ë³µìœ¼ë¡œ ì¸í•´)
```

---

## ğŸŸ  ë¬¸ì œì  15: ScrapeResultsDisplay ì»´í¬ë„ŒíŠ¸ì˜ ì œí•œì‚¬í•­

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/components/setup/ScrapeResultsDisplay.tsx` (ë¼ì¸ 1-121)

### í˜„ì¬ ì½”ë“œ
```typescript
export function ScrapeResultsDisplay({
    scrapeResults,
    onContinue,
    onRetry,
    isLoading = false
}: ScrapeResultsDisplayProps) {
    // ...
    
    {scrapeResults.results.slice(0, 5).map((result, idx) => (
        // âŒ ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ!
    ))}
    
    {scrapeResults.total_count > 5 && (
        <p className="text-xs text-gray-600 mt-4">
            ... ì™¸ {scrapeResults.total_count - 5}ê°œ ê²°ê³¼ (ëŒ€ì‹œë³´ë“œì—ì„œ ì „ì²´ í™•ì¸ ê°€ëŠ¥)
        </p>
    )}
}
```

### ë¬¸ì œì  1: ë°ì´í„° í‘œì‹œ ì œí•œ

```
ì¥ì : ì´ˆê¸° ë¡œë”© ë¹ ë¦„
ë‹¨ì : 
  âŒ ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ ì¶©ë¶„íˆ ê²€í† í•  ìˆ˜ ì—†ìŒ
  âŒ ìƒìœ„ 5ê°œ ê°€ì •ì´ í•­ìƒ ë§ì§€ ì•ŠìŒ
  âŒ í•˜ìœ„ ë°ì´í„°ë¥¼ ëŒ€ì‹œë³´ë“œì—ì„œ ì°¾ê¸° ì–´ë ¤ì›€

ì˜ˆì‹œ:
keyword: "ì„í”Œë€íŠ¸" (ìƒìœ„ 20ê°œ ê²°ê³¼)
í‘œì‹œ: 1-5ìœ„ (5ê°œ)
ìˆ¨ê¹€: 6-20ìœ„ (15ê°œ)

ì‚¬ìš©ì: "ìš°ë¦¬ ë³‘ì›ì´ 10ìœ„ì¸ë° ì™œ ì•ˆ ë³´ì—¬?"
```

### ë¬¸ì œì  2: platform ë¬¸ìì—´ í•˜ë“œì½”ë”©

```typescript
{
    scrapeResults.platform === 'NAVER_PLACE' ? 'ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤' :
    scrapeResults.platform === 'NAVER_VIEW' ? 'ë„¤ì´ë²„ VIEW' :
    scrapeResults.platform  // â† GOOGLE_ADS ë“±ì€ í‘œì‹œ ì•ˆ ë¨
}
```

### ë¬¸ì œì  3: captured_at í¬ë§·íŒ… ì˜¤ë¥˜

```typescript
<td className="py-3 px-4 text-gray-500 text-xs">
    {new Date(result.captured_at).toLocaleString('ko-KR')}
</td>
```

**ë¬¸ì œ**: ë§Œì•½ captured_atì´ ISO 8601 í˜•ì‹ì´ ì•„ë‹ˆë©´?
```javascript
new Date("2026-02-20 10:30:00")  // âŒ ì¼ë¶€ ë¸Œë¼ìš°ì €ì—ì„œ Invalid Date
new Date("2026-02-20T10:30:00Z") // âœ… ëª¨ë“  ë¸Œë¼ìš°ì €ì—ì„œ ì‘ë™
```

### ë¬¸ì œì  4: ì—ëŸ¬ ìƒíƒœ ë¯¸ì²˜ë¦¬

```typescript
// âŒ has_data === null ë˜ëŠ” results === nullì¸ ê²½ìš° ì²˜ë¦¬ ì•ˆ í•¨
if (scrapeResults.has_data && scrapeResults.results.length > 0) {
    // ì„±ê³µ
} else {
    // ì‹¤íŒ¨? ë°ì´í„° ì—†ìŒ? êµ¬ë¶„ ë¶ˆê°€
}

// ë§Œì•½ scrapeResults ìì²´ê°€ nullì´ë©´?
{scrapeResults.has_data && ...}  // âœ… ì•ˆì „
{scrapeResults.results.slice(0, 5) ...}  // âŒ null.results â†’ TypeError!
```

---

## ğŸ› ï¸ ë¬¸ì œì ë³„ ìˆ˜ì • ì „ëµ

### ìš°ì„ ìˆœìœ„ 1 (ì¦‰ì‹œ ìˆ˜ì • - ì‹œìŠ¤í…œ ì¥ì• )
1. **ë¬¸ì œì  1**: Dockerfileì— ENV ì„¤ì •
2. **ë¬¸ì œì  7**: scrape ì—”ë“œí¬ì¸íŠ¸ì— `get_current_user` ì¶”ê°€
3. **ë¬¸ì œì  11**: DailyRank.rank_change í•„ë“œ ì¶”ê°€ ë˜ëŠ” APIì—ì„œ ì œê±°
4. **ë¬¸ì œì  12**: db.rollback() ì¶”ê°€ (ë°ì´í„° ë¬´ê²°ì„±)

### ìš°ì„ ìˆœìœ„ 2 (ì£¼ìš” ê¸°ëŠ¥)
5. **ë¬¸ì œì  5**: ë™ì  ëŒ€ê¸° ì‹œê°„ êµ¬í˜„ (polling ë˜ëŠ” WebSocket)
6. **ë¬¸ì œì  6**: ì—ëŸ¬ í”Œë˜ê·¸ ì¶”ê°€ ë° ì²˜ë¦¬
7. **ë¬¸ì œì  8**: BackgroundTasks â†’ Cloud Tasks ë˜ëŠ” Celery ì „í™˜
8. **ë¬¸ì œì  14**: ë™ì‹œ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ë°©ì§€ (mutex ë˜ëŠ” ìš”ì²­ ê²€ì¦)

### ìš°ì„ ìˆœìœ„ 3 (ê°œì„ )
9. **ë¬¸ì œì  3**: ë¬¸ì„œí™” ì¶”ê°€ (ì •í™•í•œ ê²½ë¡œ ëª…ì‹œ)
10. **ë¬¸ì œì  4**: í´ë¼ì´ì–¸íŠ¸ID ê²€ì¦ ë¡œì§ ê°•í™”
11. **ë¬¸ì œì  9**: task_id ì¶”ì  API êµ¬í˜„
12. **ë¬¸ì œì  10**: Step 1ì—ì„œ ê²€ì¦ ì´ë™
13. **ë¬¸ì œì  13**: toast ë©”ì‹œì§€ ì¼ê´€ì„± ê°œì„ 
14. **ë¬¸ì œì  15**: ScrapeResultsDisplay ì»´í¬ë„ŒíŠ¸ ê°œì„ 

---

## ğŸ“ˆ ì‹œìŠ¤í…œ ì˜í–¥ë„ ë¶„ì„

```
ì‚¬ìš©ì ì—¬ì • (SetupWizard):

Step 1: ì—…ì²´ ì„ íƒ/ìƒì„±
  â”œâ”€ API_BASE_URL ë¯¸ì„¤ì • (ë¬¸ì œì  1, 2) â†’ API í˜¸ì¶œ ì‹¤íŒ¨
  â”œâ”€ createClient() ì‹¤íŒ¨ â†’ Step 2 ë¯¸ì§„í–‰
  â””â”€ âœ… Step 2ë¡œ ì§„í–‰

Step 2: íƒ€ê²Ÿ ì…ë ¥
  â”œâ”€ searchTargets() API í˜¸ì¶œ
  â”œâ”€ updateBulkTargets() API í˜¸ì¶œ
  â””â”€ âœ… Step 3ë¡œ ì§„í–‰

Step 3: ì¡°ì‚¬ ì‹œì‘
  â”œâ”€ saveAnalysisHistory() í˜¸ì¶œ
  â”œâ”€ scrapePlace() í˜¸ì¶œ
  â”‚   â”œâ”€ ì¸ì¦ ê²€ì¦ ì—†ìŒ (ë¬¸ì œì  7)
  â”‚   â”œâ”€ BackgroundTasks ì‹ ë¢°ì„± ë¶€ì¡± (ë¬¸ì œì  8)
  â”‚   â””â”€ task_id ì¶”ì  ë¶ˆê°€ (ë¬¸ì œì  9)
  â”œâ”€ 2ì´ˆ ê³ ì • ëŒ€ê¸° (ë¬¸ì œì  5)
  â”œâ”€ getScrapeResults() í˜¸ì¶œ
  â”‚   â”œâ”€ UUID ê²€ì¦ ì‹¤íŒ¨ ê°€ëŠ¥ (ë¬¸ì œì  4)
  â”‚   â”œâ”€ ê²½ë¡œ ë¯¸ì„¤ì • (ë¬¸ì œì  3)
  â”‚   â””â”€ ì—ëŸ¬ ë¬´ì‹œ (ë¬¸ì œì  6)
  â””â”€ ê²°ê³¼ í‘œì‹œ

ì¥ì•  í™•ë¥ :
API_BASE_URL: 100% (ë°°í¬ ì‹œë§ˆë‹¤)
scrapePlace: 40% (ë„¤íŠ¸ì›Œí¬/ì—ëŸ¬ ì²˜ë¦¬ ë¶€ì¡±)
getScrapeResults: 30% (íƒ€ì´ë° + ì—ëŸ¬ ì²˜ë¦¬)
ì „ì²´: 95%+ ì–´ëŠ í•œ ê°€ì§€ëŠ” ë¬¸ì œ ë°œìƒ
```

---

## ğŸ¯ ê²°ë¡ 

**í˜„ì¬ ìƒíƒœ:**
- ğŸ”´ 5ê°œ ì‹¬ê°í•œ ë¬¸ì œ (API ì‘ë™ ë¶ˆê°€, ë³´ì•ˆ ì·¨ì•½)
- ğŸŸ¡ 3ê°œ ì¤‘ê°„ ë¬¸ì œ (ë°ì´í„° ë¯¸í‘œì‹œ, ì—ëŸ¬ ì²˜ë¦¬)
- ğŸŸ  2ê°œ ë‚®ì€ ë¬¸ì œ (UX ê°œì„ )

**ìš°ì„  ì¡°ì¹˜:**
1. Dockerfile + GitHub Actions í™˜ê²½ë³€ìˆ˜ ìˆ˜ì •
2. scrape ì—”ë“œí¬ì¸íŠ¸ ì¸ì¦ ì¶”ê°€
3. SetupWizard ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”

**ë‹¤ìŒ ë‹¨ê³„:**
- BackgroundTasks â†’ Cloud Tasks ë§ˆì´ê·¸ë ˆì´ì…˜
- ë™ì  ëŒ€ê¸° ì‹œê°„ ë˜ëŠ” WebSocket í´ë§
- í¬ê´„ì  ì—ëŸ¬ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

**ì˜ˆìƒ ìˆ˜ì • ì‹œê°„:**
- ê¸´ê¸‰: 1-2ì‹œê°„
- ì£¼ìš”: 4-6ì‹œê°„
- ì „ì²´: 12-16ì‹œê°„

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-02-20
**ë‹¤ìŒ ê²€í† **: ëª¨ë“  ìˆ˜ì • í›„ í†µí•© í…ŒìŠ¤íŠ¸
