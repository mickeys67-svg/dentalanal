# ğŸ” ëŒ€ê·œëª¨ ë””ë²„ê¹…: ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ë¶„ì„

**ë¶„ì„ ì¼ì‹œ**: 2026-02-20
**ëª©í‘œ**: ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•ŠëŠ” ê·¼ë³¸ ì›ì¸ íŒŒì•…

---

## ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ íë¦„ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend (í”„ë¡ íŠ¸ì—”ë“œ): SetupWizard.tsx                          â”‚
â”‚ â†’ "ì¡°ì‚¬ ì‹œì‘" ë²„íŠ¼ í´ë¦­                                        â”‚
â”‚ â†’ POST /api/v1/scrape/place                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backend Endpoint: scrape.py                                    â”‚
â”‚ @router.post("/place")                                         â”‚
â”‚ â†’ trigger_place_scrape()                                       â”‚
â”‚ â†’ Concurrent task check (_active_scraping_tasks)              â”‚
â”‚ â†’ background_tasks.add_task(scrape_place_task)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Background Task: tasks.py                                      â”‚
â”‚ execute_place_sync()                                           â”‚
â”‚ â†’ asyncio.run(run_place_scraper(keyword))                     â”‚
â”‚ â†’ NaverPlaceScraper.get_rankings(keyword)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scraper: base.py                                               â”‚
â”‚ fetch_page_content(url)                                        â”‚
â”‚ â†’ Connect to Browser (Local or Bright Data)                   â”‚
â”‚ â†’ Navigate to Naver Maps API                                  â”‚
â”‚ â†’ Extract JSON from page                                      â”‚
â”‚ â†’ Return raw HTML/JSON                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parser: naver_place.py                                         â”‚
â”‚ Extract rankings from JSON:                                   â”‚
â”‚ data['result']['place']['list']                               â”‚
â”‚ â†’ Parse each item                                             â”‚
â”‚ â†’ Return List[dict] with rank, name, id, etc.               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database Service: analysis.py                                  â”‚
â”‚ save_place_results(keyword, results)                           â”‚
â”‚ â†’ Create/find Keyword in DB                                   â”‚
â”‚ â†’ For each result:                                            â”‚
â”‚   - Create Target if not exists                              â”‚
â”‚   - Create DailyRank record                                   â”‚
â”‚   - Link to Keyword                                           â”‚
â”‚ â†’ Commit transaction                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database: Supabase/PostgreSQL                                  â”‚
â”‚ Tables: keywords, targets, daily_ranks                         â”‚
â”‚ â†’ Data persisted                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”´ ë°ì´í„° ë¯¸ìˆ˜ì§‘ ì›ì¸ (ê°€ì„¤ ë¶„ì„)

### **ì›ì¸ 1: Bright Data CDP URL ë¯¸ì„¤ì • ë˜ëŠ” ì˜ëª»ëœ í˜•ì‹** ğŸ”´ ë†’ìŒ

**ì½”ë“œ ìœ„ì¹˜**: `base.py` ë¼ì¸ 25-44

```python
cdp_url = os.getenv("BRIGHT_DATA_CDP_URL")

if cdp_url and cdp_url.startswith("wss://"):
    # Connect to Bright Data
    browser = await p.chromium.connect_over_cdp(cdp_url)
else:
    # Fallback to local browser
    browser = await p.chromium.launch(headless=True)
```

**ë¬¸ì œì **:
- â“ `BRIGHT_DATA_CDP_URL` í™˜ê²½ë³€ìˆ˜ ì„¤ì •ë˜ì–´ ìˆëŠ”ê°€?
- â“ í˜•ì‹ì´ `wss://` ë¡œ ì‹œì‘í•˜ëŠ”ê°€?
- âŒ ë¡œì»¬ ë¸Œë¼ìš°ì €ë¡œ í´ë°± ì‹œ ì„±ê³µí•˜ëŠ”ê°€?

**í…ŒìŠ¤íŠ¸**:
```bash
# 1. í™˜ê²½ë³€ìˆ˜ í™•ì¸
echo $BRIGHT_DATA_CDP_URL

# 2. í˜•ì‹ í™•ì¸
echo "wss://" ë¡œ ì‹œì‘í•˜ëŠ”ê°€?

# 3. ë¡œì»¬ ë¸Œë¼ìš°ì € í…ŒìŠ¤íŠ¸
# â†’ Playwrightê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ê°€?
# â†’ headless=True ìƒíƒœì—ì„œ ì—°ê²° ê°€ëŠ¥í•œê°€?
```

---

### **ì›ì¸ 2: Naver Maps API ì‘ë‹µ êµ¬ì¡° ë³€ê²½** ğŸ”´ ë†’ìŒ

**ì½”ë“œ ìœ„ì¹˜**: `naver_place.py` ë¼ì¸ 23-34

```python
try:
    data = json.loads(response_text)

    # Structure: result -> place -> list
    if 'result' not in data or 'place' not in data['result']:
        self.logger.warning(f"No Place data found in API for {keyword}")
        return []

    place_list = data['result']['place']['list']
```

**ë¬¸ì œì **:
- â“ API ì‘ë‹µì— `result.place.list` ê°€ ìˆëŠ”ê°€?
- âŒ API ì‘ë‹µ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŒ
- âŒ ë¹ˆ ë°°ì—´ `[]` ë°˜í™˜í•˜ë©´ ë°ì´í„° ìˆ˜ì§‘ ì•ˆ ë¨

**í…ŒìŠ¤íŠ¸**:
```bash
# ì‹¤ì œ API ì‘ë‹µ í™•ì¸
curl "https://map.naver.com/p/api/search/allSearch?query=ì„í”Œë€íŠ¸&type=all&searchCoord=127.027610%3B37.498095"

# ë˜ëŠ” Pythonìœ¼ë¡œ
import json
import urllib.parse
keyword = "ì„í”Œë€íŠ¸"
url = f"https://map.naver.com/p/api/search/allSearch?query={urllib.parse.quote(keyword)}&type=all"
# ë¸Œë¼ìš°ì €ë¡œ ë°©ë¬¸í•´ì„œ ì‘ë‹µ í™•ì¸
```

---

### **ì›ì¸ 3: ìŠ¤í¬ë˜í•‘ ì—ëŸ¬ ë¡œê¹… ë¶€ì¬** ğŸŸ  ì¤‘ê°„

**ì½”ë“œ ìœ„ì¹˜**: `tasks.py` ë¼ì¸ 48-52

```python
try:
    results = asyncio.run(run_place_scraper(keyword))
except Exception as e:
    logger.error(f"Scraping failed for {keyword}: {e}")
    error_msg = str(e)
    results = []  # â† ì—ëŸ¬ ì‹œ ë¹ˆ ë°°ì—´ ë°˜í™˜!
```

**ë¬¸ì œì **:
- âŒ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ `results = []`
- âŒ ë¹ˆ ë°°ì—´ì´ë©´ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì•ˆ í•¨
- âŒ ì‚¬ìš©ìëŠ” "ë°ì´í„° ì—†ìŒ"ë§Œ ë³´ì„ (ì›ì¸ ëª¨ë¦„)

**ê²°ê³¼**:
```python
if results:  # â† ë¹ˆ ë°°ì—´ì´ë©´ False
    service.save_place_results(keyword, results, client_uuid)
```

---

### **ì›ì¸ 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨ (ì¡°ìš©í•œ ì‹¤íŒ¨)** ğŸŸ  ì¤‘ê°„

**ì½”ë“œ ìœ„ì¹˜**: `tasks.py` ë¼ì¸ 85-92

```python
try:
    service = AnalysisService(db)
    if results:
        service.save_place_results(keyword, results, client_uuid)
    # ... ì•Œë¦¼ ì¶”ê°€
except Exception as e:
    logger.error(f"Saving place results or notifying failed: {e}")
    db.rollback()  # â† ë¡¤ë°±ë˜ì–´ ë°ì´í„° ì†ì‹¤!
```

**ë¬¸ì œì **:
- âŒ ì €ì¥ ì‹¤íŒ¨í•´ë„ ì‚¬ìš©ìëŠ” "ì„±ê³µ"ìœ¼ë¡œ ë´„
- âŒ íŠ¸ëœì­ì…˜ ë¡¤ë°± â†’ ë°ì´í„° ì—†ìŒ
- âŒ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë°±ì—”ë“œ ë¡œê·¸ì—ë§Œ ìˆìŒ

---

### **ì›ì¸ 5: Naver Ads API ë¯¸êµ¬í˜„** ğŸŸ  ì¤‘ê°„

**ì½”ë“œ ìœ„ì¹˜**: `/api/v1/naver/ads` (naver_ads.py)

```python
# API ì—”ë“œí¬ì¸íŠ¸ë§Œ ìˆê³ 
# ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ì€ ì–´ë””ì„œ?
```

**ë¬¸ì œì **:
- â“ ê´‘ê³  ë°ì´í„°ëŠ” ì–´ë””ì„œ ìˆ˜ì§‘í•˜ëŠ”ê°€?
- âŒ `/scrape/ad` ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆë‚˜?
- âŒ Naver Ads API í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ë™ë˜ì—ˆë‚˜?

---

### **ì›ì¸ 6: Keywordì™€ Target ë¯¸ìƒì„±** ğŸŸ¡ ë‚®ìŒ

**ì½”ë“œ ìœ„ì¹˜**: `analysis.py` ë¼ì¸ 31-43

```python
def _get_or_create_keyword(self, term: str, client_id: Optional[UUID] = None) -> Keyword:
    query = self.db.query(Keyword).filter(Keyword.term == term)
    if client_id:
        query = query.filter(Keyword.client_id == client_id)

    keyword = query.first()

    if not keyword:
        keyword = Keyword(id=uuid4(), term=term, client_id=client_id)
        self.db.add(keyword)
        self.db.commit()
```

**ë¬¸ì œì **:
- â“ Keyword ìƒì„± ì‹¤íŒ¨í•˜ë©´?
- âŒ DailyRankê°€ ì–´ë””ì— ì—°ê²°ë˜ë‚˜?
- âŒ Foreign key constraint ìœ„ë°˜ ê°€ëŠ¥

---

## ğŸ§ª ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸

### Level 1: í™˜ê²½ë³€ìˆ˜ í™•ì¸

```bash
# 1ï¸âƒ£ Bright Data ì„¤ì •
echo "BRIGHT_DATA_CDP_URL: ${BRIGHT_DATA_CDP_URL:0:20}..."

# 2ï¸âƒ£ Naver API ì„¤ì •
echo "NAVER_CLIENT_ID: ${NAVER_CLIENT_ID:0:10}..."
echo "NAVER_CLIENT_SECRET: ${NAVER_CLIENT_SECRET:0:10}..."

# 3ï¸âƒ£ Naver Ads API ì„¤ì •
echo "NAVER_AD_CUSTOMER_ID: $NAVER_AD_CUSTOMER_ID"
echo "NAVER_AD_ACCESS_LICENSE: ${NAVER_AD_ACCESS_LICENSE:0:10}..."
```

### Level 2: ë°±ì—”ë“œ ë¡œê·¸ í™•ì¸

```bash
# ë°±ì—”ë“œ ì»¨í…Œì´ë„ˆ ë¡œê·¸ (Cloud Run)
gcloud run logs read dentalanal-backend --limit 100

# íŠ¹ì • ì—ëŸ¬ í•„í„°
gcloud run logs read dentalanal-backend --limit 100 | grep -i "error\|fail\|exception"
```

### Level 3: ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸

```sql
-- Supabase SQL Editorì—ì„œ

-- 1ï¸âƒ£ Keywords í…Œì´ë¸”
SELECT COUNT(*) as keyword_count FROM keywords;
SELECT * FROM keywords ORDER BY created_at DESC LIMIT 10;

-- 2ï¸âƒ£ DailyRank í…Œì´ë¸”
SELECT COUNT(*) as rank_count FROM daily_ranks;
SELECT * FROM daily_ranks ORDER BY date DESC LIMIT 10;

-- 3ï¸âƒ£ Targets í…Œì´ë¸”
SELECT COUNT(*) as target_count FROM targets;
SELECT * FROM targets LIMIT 10;

-- 4ï¸âƒ£ RawScrapingLog í…Œì´ë¸” (ì¡´ì¬í•˜ëŠ”ê°€?)
SELECT COUNT(*) FROM raw_scraping_logs;
```

### Level 4: API ì‘ë‹µ ì§ì ‘ í…ŒìŠ¤íŠ¸

```bash
# 1ï¸âƒ£ Naver Maps API í…ŒìŠ¤íŠ¸
curl -X GET "https://map.naver.com/p/api/search/allSearch?query=ì„í”Œë€íŠ¸&type=all&searchCoord=127.027610%3B37.498095" \
  -H "User-Agent: Mozilla/5.0"

# 2ï¸âƒ£ SetupWizard API í˜¸ì¶œ íë¦„
# - POST /api/v1/scrape/place (keyword="ì„í”Œë€íŠ¸", client_id="...")
# - ì‘ë‹µ í™•ì¸

# 3ï¸âƒ£ ê²°ê³¼ í´ë§
# - GET /api/v1/scrape/results (client_id, keyword, platform)
# - ë°ì´í„° ìˆëŠ”ê°€?
```

---

## ğŸ“‹ ì‹¤ì œ ì‹¤í–‰í•´ë³¼ ê²€ì‚¬ ëª…ë ¹ì–´

### Backend Health Check
```bash
# 1. ë°±ì—”ë“œ health í™•ì¸
curl https://dentalanal-backend-xxx.run.app/health
# Expected: {"status": "ok"}

# 2. Naver Ads ìˆ˜ì§‘ ë°ì´í„° í™•ì¸
curl -H "Authorization: Bearer {token}" \
  https://dentalanal-backend-xxx.run.app/api/v1/naver/collected-data
# Expected: Naver ë°ì´í„° ë°˜í™˜
```

### Database Inspection
```sql
-- Supabase Console â†’ SQL Editor

-- ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„° í™•ì¸
SELECT
  k.term,
  COUNT(dr.id) as rank_count,
  MAX(dr.date) as latest_date
FROM keywords k
LEFT JOIN daily_ranks dr ON k.id = dr.keyword_id
GROUP BY k.id, k.term
ORDER BY MAX(dr.date) DESC
LIMIT 20;

-- ì—ëŸ¬ ë¡œê·¸ í™•ì¸
SELECT * FROM raw_scraping_logs
WHERE created_at > NOW() - INTERVAL '24 hours'
ORDER BY created_at DESC;
```

### Log Analysis
```bash
# Cloud Run ë¡œê·¸ì—ì„œ "scrape" ê´€ë ¨ ë¡œê·¸
gcloud run logs read dentalanal-backend --limit 200 | grep -A5 "scrape\|place\|naver"

# ì—ëŸ¬ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì°¾ê¸°
gcloud run logs read dentalanal-backend --limit 200 | grep -A10 "Exception\|Error\|Traceback"
```

---

## ğŸ”§ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„  ì‚¬í•­

### 1ï¸âƒ£ ìŠ¤í¬ë˜í•‘ ì—ëŸ¬ ë¡œê¹… ê°•í™”

```python
# tasks.py ìˆ˜ì • ì „
try:
    results = asyncio.run(run_place_scraper(keyword))
except Exception as e:
    logger.error(f"Scraping failed for {keyword}: {e}")

# ìˆ˜ì • í›„
import traceback
try:
    results = asyncio.run(run_place_scraper(keyword))
except Exception as e:
    logger.error(f"Scraping failed for {keyword}: {e}")
    logger.error(traceback.format_exc())  # â† ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤
    # Sentry ì „ì†¡
    if sentry_sdk:
        sentry_sdk.capture_exception(e)
```

### 2ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì—ëŸ¬ ì²˜ë¦¬

```python
# í˜„ì¬: ì¡°ìš©í•œ ì‹¤íŒ¨
try:
    service.save_place_results(...)
except Exception as e:
    logger.error(f"Failed to save: {e}")

# ê°œì„ : ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
try:
    service.save_place_results(...)
except Exception as e:
    logger.error(f"Failed to save: {e}")
    # ë³´ìƒ ì¡°ì¹˜: Sentry ì•Œë¦¼, Admin í†µë³´
    sentry_sdk.capture_exception(e)
    # DLQì— ì €ì¥: Dead Letter Queueë¡œ ë‚˜ì¤‘ì— ì¬ì‹œë„
```

### 3ï¸âƒ£ API ì‘ë‹µ ê²€ì¦

```python
# naver_place.pyì— ìƒì„¸ ê²€ì¦ ì¶”ê°€
try:
    data = json.loads(response_text)
    logger.debug(f"API Response keys: {data.keys()}")

    # ì‘ë‹µ êµ¬ì¡° ê²€ì¦
    if 'result' not in data:
        logger.error(f"No 'result' key. Response: {str(data)[:200]}")
        return []

    if 'place' not in data['result']:
        logger.error(f"No 'place' key in result. Keys: {data['result'].keys()}")
        return []
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **Level 1 ê²€ì‚¬**: í™˜ê²½ë³€ìˆ˜ í™•ì¸
2. **Level 2 ê²€ì‚¬**: ë°±ì—”ë“œ ë¡œê·¸ ë¶„ì„
3. **Level 3 ê²€ì‚¬**: ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì¡°íšŒ
4. **Level 4 ê²€ì‚¬**: API ì§ì ‘ í…ŒìŠ¤íŠ¸

ê° ë‹¨ê³„ë§ˆë‹¤ ë°œê²¬ì‚¬í•­ì„ ê¸°ë¡í•˜ê³  íŒ¨í„´ ë¶„ì„

---

**ì‘ì„±ì¼**: 2026-02-20
**ë‹¤ìŒ ê²€í† **: ë””ë²„ê¹… ê²°ê³¼ ìˆ˜ì§‘ í›„
