# ğŸ”´ DentalAnal ì‹œìŠ¤í…œ - ì¶”ê°€ 5ê°€ì§€ ë¬¸ì œì  ë¶„ì„ (11-15)

> **ê³ ê¸‰ ê¸°ìˆ  ì´ìŠˆ ë¶„ì„**
>
> ì‘ì„±ì¼: 2026-02-20
> ë²”ìœ„: ë¬¸ì œì  11-15 (ì‹¬í™” ë¶„ì„)

---

## ğŸ”´ ë¬¸ì œì  11: rank_change í•„ë“œ ë¯¸êµ¬í˜„ (ë°ì´í„° ë¬´ê²°ì„±)

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/models/models.py` (ë¼ì¸ 243)
- **íŒŒì¼**: `frontend/src/components/setup/ScrapeResultsDisplay.tsx` (ë¼ì¸ 14)

### í˜„ì¬ ì½”ë“œ
```python
# backend/app/models/models.py
class DailyRank(Base):
    __tablename__ = "daily_ranks"
    id = Column(GUID, primary_key=True, default=uuid.uuid4)
    rank = Column(Integer, nullable=False)
    captured_at = Column(DateTime(timezone=True), server_default=func.now())
    # âŒ rank_change í•„ë“œê°€ ì—†ìŒ!
```

```typescript
// frontend/src/components/setup/ScrapeResultsDisplay.tsx (ë¼ì¸ 8-10)
interface ScrapeResult {
    rank: number;
    rank_change?: number;  // â† APIì—ì„œ ë°˜í™˜ë˜ì§€ ì•ŠìŒ!
    target_name: string;
    ...
}
```

```python
# backend/app/api/endpoints/analyze.py (ë¼ì¸ 933-936)
result_item = {
    "rank": r.rank,
    "rank_change": r.rank_change,  # â† AttributeError: 'DailyRank' has no attribute 'rank_change'
    ...
}
```

### ë¬¸ì œì 
```
ì‹¤ì œ íë¦„:
1ï¸âƒ£ getScrapeResults() í˜¸ì¶œ
   â””â”€ backend: GET /api/v1/analyze/scrape-results/{client_id}
2ï¸âƒ£ backendì—ì„œ DailyRank ê°ì²´ ì¡°íšŒ
3ï¸âƒ£ response êµ¬ì„±:
   result_item = {
       "rank": r.rank,
       "rank_change": r.rank_change,  # âŒ AttributeError!
       ...
   }
4ï¸âƒ£ 500 Internal Server Error ë°˜í™˜
5ï¸âƒ£ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì˜ˆì™¸ ë°œìƒ

ê²°ê³¼:
  "ì¡°ì‚¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" (ì‹¤ì œëŠ” ë°±ì—”ë“œ ì—ëŸ¬)
```

### ì¦‰ì‹œ í˜„ìƒ
```
ìš”ì²­: GET /api/v1/analyze/scrape-results/uuid
ì‘ë‹µ: 500 Internal Server Error
{
  "detail": "'DailyRank' object has no attribute 'rank_change'"
}

ì½˜ì†”: 
AttributeError: 'DailyRank' object has no attribute 'rank_change'
File "backend/app/api/endpoints/analyze.py", line 936, in get_scrape_results
    "rank_change": r.rank_change,
```

### í•´ê²° ë°©ë²•
```python
# ë°©ë²• 1: ëª¨ë¸ì— í•„ë“œ ì¶”ê°€
class DailyRank(Base):
    rank = Column(Integer, nullable=False)
    rank_change = Column(Integer, nullable=True, default=0)
    captured_at = Column(DateTime(timezone=True), ...)

# ë°©ë²• 2: APIì—ì„œ í•„ë“œ ì œê±°
result_item = {
    "rank": r.rank,
    # "rank_change": r.rank_change,  # ì œê±°
    "target_name": ...
}

# ë°©ë²• 3: rank_change ê³„ì‚° ë¡œì§ ì¶”ê°€
# ì´ì „ rankì™€ í˜„ì¬ rankì˜ ì°¨ì´ ê³„ì‚°
previous_rank = db.query(DailyRank).filter(
    DailyRank.target_id == r.target_id,
    DailyRank.keyword_id == r.keyword_id,
    DailyRank.captured_at < r.captured_at
).order_by(DailyRank.captured_at.desc()).first()

rank_change = (previous_rank.rank - r.rank) if previous_rank else 0
```

**ê¶Œì¥**: ë°©ë²• 1 (ëª¨ë¸ì— í•„ë“œ ì¶”ê°€) - ê°€ì¥ í™•ì¥ì„± ì¢‹ìŒ

---

## ğŸ”´ ë¬¸ì œì  12: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ëˆ„ìˆ˜ (ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜)

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/worker/tasks.py` (ë¼ì¸ 33-127)

### í˜„ì¬ ì½”ë“œ
```python
def execute_place_sync(keyword: str, client_id_str: str = None):
    db = SessionLocal()  # â† ì„¸ì…˜ ìƒì„±
    try:
        service = AnalysisService(db)
        if results:
            service.save_place_results(keyword, results, client_uuid)
        
        admins = db.query(User).filter(...).all()
        for admin in admins:
            note = Notification(...)
            db.add(note)
        db.commit()  # â† DB ì»¤ë°‹
        
    except Exception as e:
        logger.error(f"Saving failed: {e}")
        # âŒ db.rollback() ì—†ìŒ!
    finally:
        db.close()  # â† ì„¸ì…˜ ë‹«ê¸°
    
    return results
```

### ë¬¸ì œì 

```
ì‹œë‚˜ë¦¬ì˜¤: ë°ì´í„° ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ

1ï¸âƒ£ service.save_place_results() í˜¸ì¶œ
2ï¸âƒ£ db.add() ë° db.commit()
3ï¸âƒ£ âŒ db.flush() ì¤‘ ì—ëŸ¬ ë°œìƒ (FK ì œì•½ì¡°ê±´, íŠ¸ë¦¬ê±° ë“±)
4ï¸âƒ£ except ë¸”ë¡ìœ¼ë¡œ ì´ë™
   â””â”€ logger.error() ë¡œê·¸ë§Œ í•¨
   â””â”€ âŒ db.rollback() ì—†ìŒ!
5ï¸âƒ£ finally ë¸”ë¡ ì‹¤í–‰
   â””â”€ db.close()

ê²°ê³¼:
  - ë¶€ë¶„ì ìœ¼ë¡œ ì €ì¥ëœ ë°ì´í„° ë‚¨ìŒ (ì¼ê´€ì„± ê¹¨ì§)
  - ì„¸ì…˜ì€ ë‹«í˜”ì§€ë§Œ íŠ¸ëœì­ì…˜ ë¯¸ë¡¤ë°±
  - ë‹¤ìŒ ì„¸ì…˜ì—ì„œ ì¥ì•  ë°œìƒ ê°€ëŠ¥
  - ë°ì´í„°ë² ì´ìŠ¤ ë½(lock) ë°œìƒ ê°€ëŠ¥
```

### ë” ì‹¬ê°í•œ ë¬¸ì œ

```python
# Notification ì €ì¥ ì‹œ ì—ëŸ¬ ë°œìƒ ì˜ˆì‹œ
for admin in admins:  # â† admins ì¿¼ë¦¬ ì„±ê³µ
    note = Notification(
        user_id=admin.id,  # â† adminì´ ì‚­ì œë˜ì—ˆë‹¤ë©´?
        ...
    )
    db.add(note)

db.commit()  # âŒ FK ì œì•½ì¡°ê±´ ìœ„ë°˜
# â†’ except ë¸”ë¡ìœ¼ë¡œ ì´ë™
# â†’ db.rollback() ì—†ìŒ
# â†’ ì´ì „ì˜ save_place_results()ì—ì„œ ì¶”ê°€í•œ DailyRankëŠ” ë‚¨ìŒ!
# â†’ ë°ì´í„° ë¶ˆì¼ì¹˜!
```

### í•´ê²° ë°©ë²•
```python
def execute_place_sync(keyword: str, client_id_str: str = None):
    db = SessionLocal()
    try:
        service = AnalysisService(db)
        if results:
            service.save_place_results(keyword, results, client_uuid)
        
        # ... Notification ì¶”ê°€ ...
        
        db.commit()
    except Exception as e:
        db.rollback()  # âœ… íŠ¸ëœì­ì…˜ ë¡¤ë°±
        logger.error(f"Failed: {e}")
    finally:
        db.close()
```

---

## ğŸŸ¡ ë¬¸ì œì  13: í”„ë¡ íŠ¸ì—”ë“œ toast ë©”ì‹œì§€ ì¼ê´€ì„± ë¶€ì¡±

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 219-270)

### í˜„ì¬ ì½”ë“œ
```typescript
// Step 1: ë¶„ì„ ì´ë ¥ ì €ì¥
const historyResponse = await saveAnalysisHistory({
    client_id: newClientId!,
    keyword,
    platform
});
console.log('âœ… Analysis history saved:', historyResponse);

// Step 2: ìŠ¤í¬ë˜í•‘ íŠ¸ë¦¬ê±°
toast.info('ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...');

if (platform === 'NAVER_PLACE') {
    scrapePlace(keyword, newClientId!)
        .then(() => {
            console.log('âœ… Place scraping triggered');
        })
        .catch((err) => {
            console.error('âš ï¸ Place scraping failed:', err);
            // âŒ ì—ëŸ¬ê°€ ë°œìƒí–ˆëŠ”ë°ë„ "ê²°ê³¼ë¥¼ ìˆ˜ì§‘ ì¤‘"ì´ë¼ê³  í‘œì‹œë¨!
        });
}

// Step 3: 2ì´ˆ ëŒ€ê¸°
setTimeout(async () => {
    const results = await getScrapeResults(newClientId!, keyword, platform);
    
    if (results.has_data && results.results.length > 0) {
        setScrapeResults(results);
        setShowResults(true);
        toast.success('ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!');  // â† í•­ìƒ ì„±ê³µìœ¼ë¡œ í‘œì‹œ
    } else {
        setScrapeResults(results);
        setShowResults(true);
        // âŒ ì•„ë¬´ í† ìŠ¤íŠ¸ë„ í‘œì‹œ ì•ˆ í•¨ (ì‚¬ìš©ìëŠ” ìƒíƒœë¥¼ ëª¨ë¦„)
        toast.info('ì¡°ì‚¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‚˜íƒ€ë‚  ì˜ˆì •ì…ë‹ˆë‹¤.');
    }
}, 2000);
```

### ë¬¸ì œì 

```
ì‚¬ìš©ì ê²½í—˜:

âœ… "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤"
â³ 2ì´ˆ ëŒ€ê¸°
ğŸ“Š ê²°ê³¼ í‘œì‹œ
âœ… "ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!" (ë°ì´í„° ì—†ì–´ë„)

ë˜ëŠ”

âœ… "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤"
â³ 2ì´ˆ ëŒ€ê¸°
âŒ ì—ëŸ¬: "Failed to fetch scrape results"
ğŸ˜• ì‚¬ìš©ì: "ë­ê°€ ë¬¸ì œì•¼?"
```

### ë¯¸ìŠ¤ë§¤ì¹­ ë©”ì‹œì§€

```
ì‹¤ì œ ìƒí™© vs ë©”ì‹œì§€:

1. ìŠ¤í¬ë˜í•‘ ì¤‘ ì—ëŸ¬
   ë©”ì‹œì§€: "ì¡°ì‚¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤..."
   í˜„ì‹¤: âŒ ìŠ¤í¬ë˜í•‘ì´ ì‹¤íŒ¨í•¨

2. ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨
   ë©”ì‹œì§€: "ì¡°ì‚¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ì ì‹œ í›„ ë‚˜íƒ€ë‚  ì˜ˆì •ì…ë‹ˆë‹¤."
   í˜„ì‹¤: âŒ ê²°ê³¼ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŒ (API ì—ëŸ¬)

3. ë„¤íŠ¸ì›Œí¬ ì§€ì—°
   ë©”ì‹œì§€: "ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
   í˜„ì‹¤: â³ ì•„ì§ ìŠ¤í¬ë˜í•‘ ì¤‘
```

### í•´ê²° ë°©ë²•
```typescript
// ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€
const [scrapingError, setScrapingError] = useState<string | null>(null);
const [scrapingStatus, setScrapingStatus] = useState<'idle' | 'scraping' | 'fetching' | 'done' | 'error'>('idle');

// ì—ëŸ¬ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
.catch((err) => {
    setScrapingError(err.message);
    setScrapingStatus('error');
    toast.error(`ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: ${err.message}`);
});

// ê²°ê³¼ ì¡°íšŒ ì „ ìƒíƒœ ì—…ë°ì´íŠ¸
setScrapingStatus('fetching');

// ê²°ê³¼ì— ë”°ë¥¸ ë©”ì‹œì§€
if (results.has_data && results.results.length > 0) {
    setScrapingStatus('done');
    toast.success('ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!');
} else {
    setScrapingStatus('idle');  // ì¬ì‹œë„ ê°€ëŠ¥
    toast.warning('ë°ì´í„°ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
}
```

---

## ğŸŸ¡ ë¬¸ì œì  14: ë™ì‹œ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ì²˜ë¦¬ ë¯¸í¡

### ìœ„ì¹˜
- **íŒŒì¼**: `backend/app/api/endpoints/scrape.py` (ë¼ì¸ 11-54)
- **íŒŒì¼**: `frontend/src/components/setup/SetupWizard.tsx` (ë¼ì¸ 230-260)

### í˜„ì¬ ì½”ë“œ
```python
# backend/app/api/endpoints/scrape.py
@router.post("/place")
def trigger_place_scrape(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    task_id = str(uuid.uuid4())
    # âŒ ê°™ì€ keyword + client_idë¡œ ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸ ì•ˆ í•¨!
    background_tasks.add_task(scrape_place_task, request.keyword, request.client_id)
    
    return ScrapeResponse(task_id=task_id, message="...")
```

```typescript
// frontend/src/components/setup/SetupWizard.tsx
// âŒ ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ ì—°íƒ€í•˜ë©´?
scrapePlace(keyword, newClientId!)
scrapePlace(keyword, newClientId!)
scrapePlace(keyword, newClientId!)
// 3ê°œì˜ ë™ì¼í•œ ì‘ì—…ì´ ë™ì‹œì— ì‹¤í–‰ë¨!
```

### ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤

```
ì‚¬ìš©ìê°€ "ì¡°ì‚¬ ì‹œì‘" ë²„íŠ¼ì„ 3ë²ˆ ë¹ ë¥´ê²Œ í´ë¦­:

0ms: scrapePlace() í˜¸ì¶œ #1
5ms: scrapePlace() í˜¸ì¶œ #2
10ms: scrapePlace() í˜¸ì¶œ #3

ë°±ê·¸ë¼ìš´ë“œì—ì„œ:
Task #1: ìŠ¤í¬ë˜í•‘ ì‹œì‘ â†’ DBì— ì €ì¥
Task #2: ë™ì¼í•œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ â†’ ë™ì¼í•œ ë°ì´í„° ë‹¤ì‹œ ì €ì¥
Task #3: ë˜ ë‹¤ì‹œ... (ì¤‘ë³µ ë°ì´í„° 3ë°°!)

ê²°ê³¼:
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ 3ë°° ì‚¬ìš©
- CPU 3ë°° ì‚¬ìš©
- ë°ì´í„°ë² ì´ìŠ¤ì— ì¤‘ë³µ ë ˆì½”ë“œ 3ê°œ ì €ì¥
- ë¹„ìš© ì¦ê°€
- Naver IP ì°¨ë‹¨ ìœ„í—˜
```

### ë°ì´í„°ë² ì´ìŠ¤ ì¤‘ë³µ ë¬¸ì œ

```sql
-- ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í›„
SELECT * FROM daily_ranks 
WHERE client_id = 'UUID' 
AND keyword_id = 'UUID'
AND platform = 'NAVER_PLACE';

ê²°ê³¼:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id       â”‚ target_id â”‚ rank â”‚ date  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ uuid-1   â”‚ target-1  â”‚ 1    â”‚ 2/20  â”‚ â† ì²« ë²ˆì§¸ ìŠ¤í¬ë˜í•‘
â”‚ uuid-2   â”‚ target-1  â”‚ 1    â”‚ 2/20  â”‚ â† ì¤‘ë³µ! (ë‘ ë²ˆì§¸)
â”‚ uuid-3   â”‚ target-1  â”‚ 1    â”‚ 2/20  â”‚ â† ì¤‘ë³µ! (ì„¸ ë²ˆì§¸)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ë°ì´í„° ë¶„ì„ì—ì„œ:
SELECT AVG(rank) FROM daily_ranks ...
â†’ ì •í™•í•˜ì§€ ì•Šì€ í†µê³„ (ì¤‘ë³µìœ¼ë¡œ ì¸í•´)
```

### í•´ê²° ë°©ë²•

```python
# Backend: ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì¶”ì  í…Œì´ë¸” ì¶”ê°€
class ScrapeTask(Base):
    __tablename__ = "scrape_tasks"
    id = Column(GUID, primary_key=True)
    client_id = Column(GUID, ...)
    keyword = Column(String, ...)
    platform = Column(String, ...)
    status = Column(String)  # 'PENDING', 'RUNNING', 'DONE', 'FAILED'
    created_at = Column(DateTime, ...)
    completed_at = Column(DateTime, ...)

# Endpointì—ì„œ í™•ì¸
@router.post("/place")
def trigger_place_scrape(request: ScrapeRequest, db: Session, ...):
    # ì§„í–‰ ì¤‘ì¸ ì‘ì—… í™•ì¸
    existing = db.query(ScrapeTask).filter(
        ScrapeTask.client_id == request.client_id,
        ScrapeTask.keyword == request.keyword,
        ScrapeTask.status.in_(['PENDING', 'RUNNING'])
    ).first()
    
    if existing:
        return {"status": "ALREADY_RUNNING", "task_id": str(existing.id)}
    
    # ìƒˆ ì‘ì—… ìƒì„± ë° ë“±ë¡
    task = ScrapeTask(...)
    db.add(task)
    db.commit()
    
    background_tasks.add_task(scrape_place_task, ...)
    return {"status": "SUCCESS", "task_id": str(task.id)}
```

```typescript
// Frontend: ë²„íŠ¼ ë¹„í™œì„±í™”
const [isScrapingInProgress, setIsScrapingInProgress] = useState(false);

const handleScrape = async () => {
    if (isScrapingInProgress) {
        toast.warning('ì´ë¯¸ ì¡°ì‚¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.');
        return;  // âœ… ì¤‘ë³µ ìš”ì²­ ë°©ì§€
    }
    
    setIsScrapingInProgress(true);
    try {
        await scrapePlace(keyword, newClientId!);
    } finally {
        setIsScrapingInProgress(false);
    }
};

// HTML
<button disabled={isScrapingInProgress} onClick={handleScrape}>
    ì¡°ì‚¬ ì‹œì‘
</button>
```

---

## ğŸŸ  ë¬¸ì œì  15: ScrapeResultsDisplay ì»´í¬ë„ŒíŠ¸ì˜ ì œí•œì‚¬í•­

### ìœ„ì¹˜
- **íŒŒì¼**: `frontend/src/components/setup/ScrapeResultsDisplay.tsx` (ë¼ì¸ 1-121)

### í˜„ì¬ ì½”ë“œ
```typescript
export function ScrapeResultsDisplay({
    scrapeResults,
    onContinue,
    onRetry,
    isLoading = false
}: ScrapeResultsDisplayProps) {
    // ...
    
    {scrapeResults.results.slice(0, 5).map((result, idx) => (
        // âŒ ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ!
    ))}
    
    {scrapeResults.total_count > 5 && (
        <p className="text-xs text-gray-600 mt-4">
            ... ì™¸ {scrapeResults.total_count - 5}ê°œ ê²°ê³¼ (ëŒ€ì‹œë³´ë“œì—ì„œ ì „ì²´ í™•ì¸ ê°€ëŠ¥)
        </p>
    )}
}
```

### ë¬¸ì œì  1: ë°ì´í„° í‘œì‹œ ì œí•œ

```
ì¥ì : ì´ˆê¸° ë¡œë”© ë¹ ë¦„
ë‹¨ì : 
  âŒ ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ ì¶©ë¶„íˆ ê²€í† í•  ìˆ˜ ì—†ìŒ
  âŒ ìƒìœ„ 5ê°œ ê°€ì •ì´ í•­ìƒ ë§ì§€ ì•ŠìŒ
  âŒ í•˜ìœ„ ë°ì´í„°ë¥¼ ëŒ€ì‹œë³´ë“œì—ì„œ ì°¾ê¸° ì–´ë ¤ì›€

ì˜ˆì‹œ:
keyword: "ì„í”Œë€íŠ¸" (ìƒìœ„ 20ê°œ ê²°ê³¼)
í‘œì‹œ: 1-5ìœ„ (5ê°œ)
ìˆ¨ê¹€: 6-20ìœ„ (15ê°œ)

ì‚¬ìš©ì: "ìš°ë¦¬ ë³‘ì›ì´ 10ìœ„ì¸ë° ì™œ ì•ˆ ë³´ì—¬?"
```

### ë¬¸ì œì  2: platform ë¬¸ìì—´ í•˜ë“œì½”ë”©

```typescript
{
    scrapeResults.platform === 'NAVER_PLACE' ? 'ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤' :
    scrapeResults.platform === 'NAVER_VIEW' ? 'ë„¤ì´ë²„ VIEW' :
    scrapeResults.platform  // â† GOOGLE_ADS ë“±ì€ í‘œì‹œ ì•ˆ ë¨
}
```

### ë¬¸ì œì  3: captured_at í¬ë§·íŒ… ì˜¤ë¥˜

```typescript
<td className="py-3 px-4 text-gray-500 text-xs">
    {new Date(result.captured_at).toLocaleString('ko-KR')}
</td>
```

**ë¬¸ì œ**: ë§Œì•½ captured_atì´ ISO 8601 í˜•ì‹ì´ ì•„ë‹ˆë©´?
```javascript
new Date("2026-02-20 10:30:00")  // âŒ ì¼ë¶€ ë¸Œë¼ìš°ì €ì—ì„œ Invalid Date
new Date("2026-02-20T10:30:00Z") // âœ… ëª¨ë“  ë¸Œë¼ìš°ì €ì—ì„œ ì‘ë™
```

### ë¬¸ì œì  4: ì—ëŸ¬ ìƒíƒœ ë¯¸ì²˜ë¦¬

```typescript
// âŒ has_data === null ë˜ëŠ” results === nullì¸ ê²½ìš° ì²˜ë¦¬ ì•ˆ í•¨
if (scrapeResults.has_data && scrapeResults.results.length > 0) {
    // ì„±ê³µ
} else {
    // ì‹¤íŒ¨? ë°ì´í„° ì—†ìŒ? êµ¬ë¶„ ë¶ˆê°€
}

// ë§Œì•½ scrapeResults ìì²´ê°€ nullì´ë©´?
{scrapeResults.has_data && ...}  // âœ… ì•ˆì „
{scrapeResults.results.slice(0, 5) ...}  // âŒ null.results â†’ TypeError!
```

### í•´ê²° ë°©ë²•

```typescript
// 1. Platform ë§µ ìƒì„± (ìƒìˆ˜)
const PLATFORM_NAMES: Record<string, string> = {
    'NAVER_PLACE': 'ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤',
    'NAVER_VIEW': 'ë„¤ì´ë²„ VIEW',
    'GOOGLE_ADS': 'êµ¬ê¸€ ê´‘ê³ ',
    'META_ADS': 'ë©”íƒ€ ê´‘ê³ ',
};

// 2. ì•ˆì „í•œ ë‚ ì§œ í¬ë§·íŒ…
const formatDate = (dateString: string) => {
    try {
        return new Date(dateString).toLocaleString('ko-KR');
    } catch {
        return dateString;
    }
};

// 3. í‘œì‹œ ê°œìˆ˜ ë™ì  ì¡°ì •
const DISPLAY_COUNT = Math.min(10, scrapeResults.results.length);

// 4. null ì²´í¬ ê°•í™”
if (!scrapeResults || !scrapeResults.results) {
    return <ErrorState message="ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." />;
}
```

---

## ğŸ“Š 15ê°€ì§€ ë¬¸ì œì  ì „ì²´ ìš”ì•½

| # | ë¬¸ì œ | ì‹¬ê°ë„ | ì¹´í…Œê³ ë¦¬ | ìˆ˜ì • ë‚œì´ë„ |
|---|------|--------|----------|-----------|
| 1 | NEXT_PUBLIC_API_URL | ğŸ”´ | ë°°í¬ | ë‚®ìŒ |
| 2 | Docker ëŸ°íƒ€ì„ í™˜ê²½ë³€ìˆ˜ | ğŸ”´ | ë°°í¬ | ë‚®ìŒ |
| 3 | API ë¼ìš°íŒ… ê²½ë¡œ | ğŸŸ¡ | ë¼ìš°íŒ… | ë‚®ìŒ |
| 4 | UUID ê²€ì¦ ì‹¤íŒ¨ | ğŸŸ¡ | ë°ì´í„° | ë‚®ìŒ |
| 5 | ìŠ¤í¬ë˜í•‘ ëŒ€ê¸° ì‹œê°„ | ğŸŸ¡ | ë¹„ë™ê¸° | ì¤‘ê°„ |
| 6 | ì—ëŸ¬ ë¬´ì‹œ | ğŸ”´ | ì—ëŸ¬ì²˜ë¦¬ | ì¤‘ê°„ |
| 7 | ì¸ì¦ ë¯¸ê²€ì¦ | ğŸ”´ | ë³´ì•ˆ | ë‚®ìŒ |
| 8 | BackgroundTasks | ğŸ”´ | ì•„í‚¤í…ì²˜ | ë†’ìŒ |
| 9 | ì‘ì—… ì¶”ì  ë¶ˆê°€ | ğŸŸ  | ëª¨ë‹ˆí„°ë§ | ì¤‘ê°„ |
| 10 | ê²€ì¦ ì§€ì—° | ğŸŸ  | UX | ë‚®ìŒ |
| 11 | rank_change ë¯¸êµ¬í˜„ | ğŸ”´ | ë°ì´í„° | ë‚®ìŒ |
| 12 | ì„¸ì…˜ ëˆ„ìˆ˜ | ğŸ”´ | ë¦¬ì†ŒìŠ¤ | ë‚®ìŒ |
| 13 | toast ë©”ì‹œì§€ | ğŸŸ¡ | UX | ì¤‘ê°„ |
| 14 | ë™ì‹œ ìš”ì²­ | ğŸŸ¡ | ë™ì‹œì„± | ì¤‘ê°„ |
| 15 | ì»´í¬ë„ŒíŠ¸ ì œí•œ | ğŸŸ  | UX | ë‚®ìŒ |

---

## ğŸ¯ ìš°ì„ ìˆœìœ„ ì¬ì •ì˜ (15ê°œ ê¸°ì¤€)

### Phase 1: ê¸´ê¸‰ ìˆ˜ì • (1-2ì‹œê°„)
1. **ë¬¸ì œì  1, 2**: Dockerfile & GitHub Actions í™˜ê²½ë³€ìˆ˜
2. **ë¬¸ì œì  7**: scrape ì—”ë“œí¬ì¸íŠ¸ ì¸ì¦ ì¶”ê°€
3. **ë¬¸ì œì  11**: rank_change í•„ë“œ ì¶”ê°€
4. **ë¬¸ì œì  12**: db.rollback() ì¶”ê°€

### Phase 2: ë†’ì€ ìš°ì„ ìˆœìœ„ (4-6ì‹œê°„)
5. **ë¬¸ì œì  5**: ë™ì  ëŒ€ê¸° ì‹œê°„
6. **ë¬¸ì œì  6**: ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
7. **ë¬¸ì œì  14**: ë™ì‹œ ìš”ì²­ ë°©ì§€

### Phase 3: ë³´ì™„ (6-8ì‹œê°„)
8. **ë¬¸ì œì  8**: BackgroundTasks â†’ Cloud Tasks
9. **ë¬¸ì œì  13**: toast ë©”ì‹œì§€ ê°œì„ 
10. **ë¬¸ì œì  15**: ì»´í¬ë„ŒíŠ¸ ê°œì„ 

### Phase 4: ìµœì í™” (í›„ì†)
11. **ë¬¸ì œì  3, 4, 9, 10**: ë¬¸ì„œí™” ë° ê°œì„ 

---

**ì´ ìˆ˜ì • ì˜ˆìƒ ì‹œê°„**: 16-24ì‹œê°„
**íŒ€ ê·œëª¨**: 2ëª… (í”„ë¡ íŠ¸ì—”ë“œ 1ëª…, ë°±ì—”ë“œ 1ëª…)
**ë³‘ë ¬ ì‘ì—…**: ìµœëŒ€ 4ê°œ ë¬¸ì œì  ë™ì‹œ ì‘ì—… ê°€ëŠ¥

