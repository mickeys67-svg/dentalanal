from app.core.celery_app import celery_app
from app.scrapers.naver_place import NaverPlaceScraper
from app.scrapers.naver_view import NaverViewScraper
from app.scrapers.safe_wrapper import SafeScraperWrapper
from app.schemas.response import ResponseStatus
import asyncio
import logging

logger = logging.getLogger("worker")

async def run_place_scraper(keyword: str):
    scraper = NaverPlaceScraper()
    wrapper = SafeScraperWrapper(scraper)
    response = await wrapper.run("get_rankings", keyword)
    
    if response.status == ResponseStatus.SUCCESS and response.data:
        return response.data
    # On failure, SafeWrapper has already logged to Sentry.
    # We return empty list to maintain compatibility with existing DB logic.
    return []

async def run_view_scraper(keyword: str):
    scraper = NaverViewScraper()
    wrapper = SafeScraperWrapper(scraper)
    response = await wrapper.run("get_rankings", keyword)
    
    if response.status == ResponseStatus.SUCCESS and response.data:
        return response.data
    return []

def execute_place_sync(keyword: str, client_id_str: str = None):
    """Inline execution of place scraping and saving."""
    import asyncio
    import logging
    from app.core.database import SessionLocal
    from app.services.analysis import AnalysisService
    from app.models.models import Notification, User, UserRole, MetricsDaily, Campaign, PlatformConnection, PlatformType
    from uuid import UUID, uuid4
    import datetime
    import random

    logger = logging.getLogger("worker")
    error_msg = None
    
    # Convert string ID to UUID safely
    client_uuid = UUID(client_id_str) if client_id_str else None
    
    try:
        # Use asyncio.run for a fresh loop in this thread
        results = asyncio.run(run_place_scraper(keyword))
    except Exception as e:
        logger.error(f"Scraping failed for {keyword}: {e}")
        error_msg = str(e)
        results = []

    db = SessionLocal()
    try:
        service = AnalysisService(db)
        if results:
            service.save_place_results(keyword, results, client_uuid)
            
            # ğŸ†• Also create sample metrics data for dashboard visibility
            if client_uuid:
                conn = db.query(PlatformConnection).filter(
                    PlatformConnection.client_id == client_uuid,
                    PlatformConnection.platform == PlatformType.NAVER_PLACE
                ).first()
                
                if not conn:
                    # Create a default connection if not exists
                    conn = PlatformConnection(
                        id=uuid4(),
                        client_id=client_uuid,
                        platform=PlatformType.NAVER_PLACE,
                        credentials={"auto_created": True},
                        status="ACTIVE"
                    )
                    db.add(conn)
                    db.flush()
                
                # Create or get campaign for this keyword
                campaign = db.query(Campaign).filter(
                    Campaign.connection_id == conn.id,
                    Campaign.name.ilike(f"%{keyword}%")
                ).first()
                
                if not campaign:
                    campaign = Campaign(
                        id=uuid4(),
                        connection_id=conn.id,
                        external_id=f"scrape_{keyword}_{datetime.date.today()}",
                        name=f"Scrape: {keyword}"
                    )
                    db.add(campaign)
                    db.flush()
                
                # Create metrics entry with sample data based on scraping results
                metric_entry = MetricsDaily(
                    id=uuid4(),
                    campaign_id=campaign.id,
                    date=datetime.date.today(),
                    impressions=len(results) * 100,  # Sample: 100 per result
                    clicks=len(results) * 10,         # Sample: 10 per result
                    conversions=max(1, len(results) // 5),  # Sample: conversion ratio
                    spend=len(results) * 5000,        # Sample: 5k per result
                    revenue=max(1, len(results) // 5) * 150000,  # 150k per conversion
                    source="SCRAPER"
                )
                db.add(metric_entry)
                logger.info(f"ğŸ“Š Created sample metrics for '{keyword}' scraping")
        
        # Notify Admins
        admins = db.query(User).filter(User.role.in_([UserRole.SUPER_ADMIN, UserRole.ADMIN])).all()
        count = len(results) if results else 0
        
        if error_msg:
            title = "í”Œë ˆì´ìŠ¤ ì¡°ì‚¬ ì‹¤íŒ¨"
            content = f"'{keyword}' í‚¤ì›Œë“œ ì¡°ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
        elif count == 0:
             title = "í”Œë ˆì´ìŠ¤ ì¡°ì‚¬ ê²°ê³¼ ì—†ìŒ"
             content = f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ë°ì´í„°ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (0ê±´)"
        else:
            title = "í”Œë ˆì´ìŠ¤ ì¡°ì‚¬ ì™„ë£Œ"
            content = f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆ˜ì§‘: {count}ê±´)"

        for admin in admins:
            note = Notification(
                id=uuid4(),
                user_id=admin.id,
                title=title,
                content=content,
                type="NOTICE",
                is_read=0
            )
            db.add(note)
        db.commit()
        
        logger.info(f"Place scrape finished for {keyword}. Items: {count}")
        
    except Exception as e:
        logger.error(f"Saving place results or notifying failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        db.close()
    return results

def execute_view_sync(keyword: str, client_id_str: str = None):
    """Inline execution of view scraping and saving."""
    import asyncio
    import logging
    from app.core.database import SessionLocal
    from app.services.analysis import AnalysisService
    from app.models.models import Notification, User, UserRole, MetricsDaily, Campaign, PlatformConnection, PlatformType
    from uuid import UUID, uuid4
    import datetime
    
    logger = logging.getLogger("worker")
    error_msg = None
    
    client_uuid = UUID(client_id_str) if client_id_str else None

    try:
        results = asyncio.run(run_view_scraper(keyword))
    except Exception as e:
        logger.error(f"View scraping failed for {keyword}: {e}")
        error_msg = str(e)
        results = []
    
    db = SessionLocal()
    try:
        service = AnalysisService(db)
        if results:
            service.save_view_results(keyword, results, client_uuid)
            
            # ğŸ†• Also create sample metrics data for dashboard visibility
            if client_uuid:
                conn = db.query(PlatformConnection).filter(
                    PlatformConnection.client_id == client_uuid,
                    PlatformConnection.platform == PlatformType.NAVER_VIEW
                ).first()
                
                if not conn:
                    conn = PlatformConnection(
                        id=uuid4(),
                        client_id=client_uuid,
                        platform=PlatformType.NAVER_VIEW,
                        credentials={"auto_created": True},
                        status="ACTIVE"
                    )
                    db.add(conn)
                    db.flush()
                
                campaign = db.query(Campaign).filter(
                    Campaign.connection_id == conn.id,
                    Campaign.name.ilike(f"%{keyword}%")
                ).first()
                
                if not campaign:
                    campaign = Campaign(
                        id=uuid4(),
                        connection_id=conn.id,
                        external_id=f"scrape_{keyword}_{datetime.date.today()}",
                        name=f"View Scrape: {keyword}"
                    )
                    db.add(campaign)
                    db.flush()
                
                metric_entry = MetricsDaily(
                    id=uuid4(),
                    campaign_id=campaign.id,
                    date=datetime.date.today(),
                    impressions=len(results) * 100,
                    clicks=len(results) * 10,
                    conversions=max(1, len(results) // 5),
                    spend=len(results) * 5000,
                    revenue=max(1, len(results) // 5) * 150000,
                    source="SCRAPER"
                )
                db.add(metric_entry)
                logger.info(f"ğŸ“Š Created sample metrics for '{keyword}' view scraping")
        
        # Notify Admins
        admins = db.query(User).filter(User.role.in_([UserRole.SUPER_ADMIN, UserRole.ADMIN])).all()
        count = len(results) if results else 0
        
        if error_msg:
            title = "VIEW ì¡°ì‚¬ ì‹¤íŒ¨"
            content = f"'{keyword}' í‚¤ì›Œë“œ ì¡°ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
        elif count == 0:
             title = "VIEW ì¡°ì‚¬ ê²°ê³¼ ì—†ìŒ"
             content = f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ë°ì´í„°ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (0ê±´)"
        else:
            title = "VIEW ì¡°ì‚¬ ì™„ë£Œ"
            content = f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆ˜ì§‘: {count}ê±´)"

        for admin in admins:
            note = Notification(
                id=uuid4(),
                user_id=admin.id,
                title=title,
                content=content,
                type="NOTICE",
                is_read=0
            )
            db.add(note)
        db.commit()

        logger.info(f"View scrape finished for {keyword}. Items: {count}")
    except Exception as e:
        logger.error(f"Saving view results failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        db.close()
    return results

def execute_ad_sync(keyword: str, client_id_str: str = None):
    """Inline execution of ad scraping and saving."""
    from app.scrapers.naver_ad import NaverAdScraper
    import asyncio
    import logging
    from app.core.database import SessionLocal
    from app.services.analysis import AnalysisService
    from app.models.models import Notification, User, UserRole, MetricsDaily, Campaign, PlatformConnection, PlatformType
    from uuid import UUID, uuid4
    import datetime
    
    logger = logging.getLogger("worker")
    scraper = NaverAdScraper()
    wrapper = SafeScraperWrapper(scraper)
    error_msg = None
    
    client_uuid = UUID(client_id_str) if client_id_str else None

    try:
        response = asyncio.run(wrapper.run("get_ad_rankings", keyword))
        if response.status == ResponseStatus.SUCCESS and response.data:
            results = response.data
        else:
            results = []
            if response.message:
                error_msg = response.message # Capture wrapper error message if any
    except Exception as e:
        logger.error(f"Ad scraping failed for {keyword}: {e}")
        error_msg = str(e)
        results = []
    
    db = SessionLocal()
    try:
        service = AnalysisService(db)
        if results:
            service.save_ad_results(keyword, results, client_uuid)
            
            # ğŸ†• Also create sample metrics data for dashboard visibility
            if client_uuid:
                conn = db.query(PlatformConnection).filter(
                    PlatformConnection.client_id == client_uuid,
                    PlatformConnection.platform == PlatformType.NAVER_AD
                ).first()
                
                if not conn:
                    conn = PlatformConnection(
                        id=uuid4(),
                        client_id=client_uuid,
                        platform=PlatformType.NAVER_AD,
                        credentials={"auto_created": True},
                        status="ACTIVE"
                    )
                    db.add(conn)
                    db.flush()
                
                campaign = db.query(Campaign).filter(
                    Campaign.connection_id == conn.id,
                    Campaign.name.ilike(f"%{keyword}%")
                ).first()
                
                if not campaign:
                    campaign = Campaign(
                        id=uuid4(),
                        connection_id=conn.id,
                        external_id=f"scrape_{keyword}_{datetime.date.today()}",
                        name=f"Ad Scrape: {keyword}"
                    )
                    db.add(campaign)
                    db.flush()
                
                metric_entry = MetricsDaily(
                    id=uuid4(),
                    campaign_id=campaign.id,
                    date=datetime.date.today(),
                    impressions=len(results) * 100,
                    clicks=len(results) * 10,
                    conversions=max(1, len(results) // 5),
                    spend=len(results) * 5000,
                    revenue=max(1, len(results) // 5) * 150000,
                    source="SCRAPER"
                )
                db.add(metric_entry)
                logger.info(f"ğŸ“Š Created sample metrics for '{keyword}' ad scraping")
        
        # Notify Admins
        admins = db.query(User).filter(User.role.in_([UserRole.SUPER_ADMIN, UserRole.ADMIN])).all()
        count = len(results) if results else 0
        
        if error_msg:
            title = "ê´‘ê³  ì¡°ì‚¬ ì‹¤íŒ¨"
            content = f"'{keyword}' í‚¤ì›Œë“œ ì¡°ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
        elif count == 0:
             title = "ê´‘ê³  ì¡°ì‚¬ ê²°ê³¼ ì—†ìŒ"
             content = f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ë°ì´í„°ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (0ê±´)"
        else:
            title = "ê´‘ê³  ì¡°ì‚¬ ì™„ë£Œ"
            content = f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆ˜ì§‘: {count}ê±´)"

        for admin in admins:
            note = Notification(
                id=uuid4(),
                user_id=admin.id,
                title=title,
                content=content,
                type="NOTICE",
                is_read=0
            )
            db.add(note)
        db.commit()

        logger.info(f"Ad scrape finished for {keyword}. Items: {count}")
    except Exception as e:
        logger.error(f"Saving ad results failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        db.close()
    return results

def scrape_place_task(keyword: str, client_id: str = None):
    return execute_place_sync(keyword, client_id)

def scrape_view_task(keyword: str, client_id: str = None):
    return execute_view_sync(keyword, client_id)

def scrape_ad_task(keyword: str, client_id: str = None):
    return execute_ad_sync(keyword, client_id)
